"""
å•æ ·æœ¬å›¾åƒè¯†åˆ« - ä½¿ç”¨CLIPé¢„è®­ç»ƒæ¨¡å‹ï¼ˆæ¨èæ–¹æ³•ï¼‰
CLIPæ¨¡å‹å·²ç»åœ¨å¤§é‡æ•°æ®ä¸Šè®­ç»ƒï¼Œå¯ä»¥ç›´æ¥ç”¨äºå›¾åƒç›¸ä¼¼åº¦æ¯”è¾ƒ
"""

import torch
import torch.nn.functional as F
from PIL import Image
import clip
import os
import glob
from pathlib import Path

# å°è¯•å¯¼å…¥tqdmï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç®€å•çš„è¿›åº¦æ˜¾ç¤º
try:
    from tqdm import tqdm
except ImportError:
    def tqdm(iterable, desc=""):
        """ç®€å•çš„è¿›åº¦æ¡æ›¿ä»£"""
        print(f"{desc}...")
        return iterable


class CLIPOneShotRecognizer:
    """åŸºäºCLIPçš„å•æ ·æœ¬/å¤šæ ·æœ¬è¯†åˆ«å™¨ï¼ˆæ¨èä½¿ç”¨ï¼‰"""
    
    def __init__(self, model_name='ViT-B/32'):
        """
        åˆå§‹åŒ–CLIPæ¨¡å‹
        
        Args:
            model_name: CLIPæ¨¡å‹åç§°ï¼Œå¯é€‰ï¼š
                - 'ViT-B/32' (æ¨èï¼Œé€Ÿåº¦å¿«)
                - 'ViT-B/16' (ç²¾åº¦æ›´é«˜)
                - 'ViT-L/14' (ç²¾åº¦æœ€é«˜ï¼Œä½†æ›´æ…¢)
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"æ­£åœ¨åŠ è½½CLIPæ¨¡å‹: {model_name}...")
        
        # åŠ è½½CLIPæ¨¡å‹å’Œé¢„å¤„ç†
        self.model, self.preprocess = clip.load(model_name, device=self.device)
        self.model.eval()
        
        # å­˜å‚¨è®­ç»ƒåçš„ç‰¹å¾
        self.trained_features = None
        self.train_image_paths = []
        
        # å¤šç±»åˆ«è®­ç»ƒçš„ç‰¹å¾å­˜å‚¨
        self.class_features = {}  # {ç±»åˆ«å: ç‰¹å¾å‘é‡}
        self.class_image_counts = {}  # {ç±»åˆ«å: å›¾ç‰‡æ•°é‡}
        
        print("CLIPæ¨¡å‹åŠ è½½å®Œæˆï¼")
    
    def extract_features(self, image_path):
        """
        æå–å›¾ç‰‡ç‰¹å¾
        
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
        
        Returns:
            torch.Tensor: ç‰¹å¾å‘é‡
        """
        image = Image.open(image_path).convert('RGB')
        image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            # æå–å›¾åƒç‰¹å¾
            image_features = self.model.encode_image(image_tensor)
            # å½’ä¸€åŒ–ç‰¹å¾å‘é‡
            image_features = F.normalize(image_features, dim=1)
        
        return image_features
    
    def recognize(self, train_image_path, test_image_path, threshold=0.7):
        """
        è¯†åˆ«æµ‹è¯•å›¾ç‰‡æ˜¯å¦ä¸è®­ç»ƒå›¾ç‰‡åŒ¹é…
        
        Args:
            train_image_path: è®­ç»ƒå›¾ç‰‡è·¯å¾„
            test_image_path: æµ‹è¯•å›¾ç‰‡è·¯å¾„
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ä¹‹é—´ï¼Œé»˜è®¤0.7ï¼‰
        
        Returns:
            bool: æ˜¯å¦åŒ¹é…
            float: ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼ŒèŒƒå›´0-1ï¼‰
        """
        if not os.path.exists(train_image_path):
            raise FileNotFoundError(f"è®­ç»ƒå›¾ç‰‡ä¸å­˜åœ¨: {train_image_path}")
        if not os.path.exists(test_image_path):
            raise FileNotFoundError(f"æµ‹è¯•å›¾ç‰‡ä¸å­˜åœ¨: {test_image_path}")
        
        # æå–ç‰¹å¾
        train_features = self.extract_features(train_image_path)
        test_features = self.extract_features(test_image_path)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarity = F.cosine_similarity(train_features, test_features).item()
        
        # åˆ¤æ–­æ˜¯å¦åŒ¹é…
        is_match = similarity >= threshold
        
        return is_match, similarity
    
    def recognize_batch(self, train_image_path, test_image_paths, threshold=0.7):
        """
        æ‰¹é‡è¯†åˆ«å¤šå¼ æµ‹è¯•å›¾ç‰‡
        
        Args:
            train_image_path: è®­ç»ƒå›¾ç‰‡è·¯å¾„
            test_image_paths: æµ‹è¯•å›¾ç‰‡è·¯å¾„åˆ—è¡¨
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        
        Returns:
            list: [(æ˜¯å¦åŒ¹é…, ç›¸ä¼¼åº¦åˆ†æ•°), ...]
        """
        train_features = self.extract_features(train_image_path)
        results = []
        
        for test_path in test_image_paths:
            if not os.path.exists(test_path):
                results.append((False, 0.0))
                continue
            
            test_features = self.extract_features(test_path)
            similarity = F.cosine_similarity(train_features, test_features).item()
            is_match = similarity >= threshold
            results.append((is_match, similarity))
        
        return results
    
    def train_from_images(self, train_image_paths, aggregation='mean'):
        """
        ä½¿ç”¨å¤šå¼ å›¾ç‰‡è¿›è¡Œè®­ç»ƒï¼Œæå–å¹¶èšåˆç‰¹å¾
        
        Args:
            train_image_paths: è®­ç»ƒå›¾ç‰‡è·¯å¾„åˆ—è¡¨
            aggregation: ç‰¹å¾èšåˆæ–¹å¼ï¼Œå¯é€‰ï¼š
                - 'mean': å¹³å‡ç‰¹å¾ï¼ˆé»˜è®¤ï¼Œæ¨èï¼‰
                - 'max': æœ€å¤§ç‰¹å¾
                - 'all': ä¿ç•™æ‰€æœ‰ç‰¹å¾ï¼ˆåŒ¹é…æ—¶å–æœ€é«˜ç›¸ä¼¼åº¦ï¼‰
        
        Returns:
            int: æˆåŠŸå¤„ç†çš„å›¾ç‰‡æ•°é‡
        """
        if not train_image_paths:
            raise ValueError("è®­ç»ƒå›¾ç‰‡åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        print(f"\næ­£åœ¨ä» {len(train_image_paths)} å¼ å›¾ç‰‡æå–ç‰¹å¾...")
        
        features_list = []
        valid_paths = []
        
        for img_path in tqdm(train_image_paths, desc="æå–ç‰¹å¾"):
            if not os.path.exists(img_path):
                print(f"\nè­¦å‘Š: å›¾ç‰‡ä¸å­˜åœ¨ï¼Œè·³è¿‡: {img_path}")
                continue
            try:
                features = self.extract_features(img_path)
                features_list.append(features)
                valid_paths.append(img_path)
            except Exception as e:
                print(f"\nè­¦å‘Š: å¤„ç†å¤±è´¥ï¼Œè·³è¿‡ {img_path}: {str(e)}")
                continue
        
        if len(features_list) == 0:
            raise ValueError("æ²¡æœ‰æˆåŠŸæå–ä»»ä½•ç‰¹å¾ï¼è¯·æ£€æŸ¥å›¾ç‰‡è·¯å¾„ã€‚")
        
        self.train_image_paths = valid_paths
        
        # èšåˆç‰¹å¾
        if aggregation == 'mean':
            # è®¡ç®—å¹³å‡ç‰¹å¾
            all_features = torch.cat(features_list, dim=0)
            self.trained_features = F.normalize(all_features.mean(dim=0, keepdim=True), dim=1)
            print(f"\nå·²ä½¿ç”¨å¹³å‡èšåˆæ–¹å¼è®­ç»ƒï¼Œå…± {len(valid_paths)} å¼ å›¾ç‰‡")
        elif aggregation == 'max':
            # è®¡ç®—æœ€å¤§ç‰¹å¾
            all_features = torch.cat(features_list, dim=0)
            self.trained_features = F.normalize(all_features.max(dim=0, keepdim=True)[0], dim=1)
            print(f"\nå·²ä½¿ç”¨æœ€å¤§èšåˆæ–¹å¼è®­ç»ƒï¼Œå…± {len(valid_paths)} å¼ å›¾ç‰‡")
        elif aggregation == 'all':
            # ä¿ç•™æ‰€æœ‰ç‰¹å¾
            self.trained_features = torch.cat(features_list, dim=0)
            print(f"\nå·²ä¿ç•™æ‰€æœ‰ç‰¹å¾ï¼Œå…± {len(valid_paths)} å¼ å›¾ç‰‡")
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„èšåˆæ–¹å¼: {aggregation}")
        
        return len(valid_paths)
    
    def train_from_directory(self, train_dir, aggregation='mean', max_images=None):
        """
        ä»ç›®å½•åŠ è½½å›¾ç‰‡è¿›è¡Œè®­ç»ƒ
        
        Args:
            train_dir: è®­ç»ƒå›¾ç‰‡ç›®å½•è·¯å¾„
            aggregation: ç‰¹å¾èšåˆæ–¹å¼ ('mean', 'max', 'all')
            max_images: æœ€å¤šä½¿ç”¨çš„å›¾ç‰‡æ•°é‡ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
        
        Returns:
            int: æˆåŠŸå¤„ç†çš„å›¾ç‰‡æ•°é‡
        """
        if not os.path.exists(train_dir):
            raise FileNotFoundError(f"è®­ç»ƒç›®å½•ä¸å­˜åœ¨: {train_dir}")
        
        # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
        image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.gif', '*.JPG', '*.JPEG', '*.PNG']
        
        # è·å–ç›®å½•ä¸­æ‰€æœ‰å›¾ç‰‡ï¼ˆé€’å½’ï¼‰
        image_paths = []
        for ext in image_extensions:
            image_paths.extend(glob.glob(os.path.join(train_dir, ext)))
            image_paths.extend(glob.glob(os.path.join(train_dir, '**', ext), recursive=True))
        
        # å»é‡å¹¶æ’åº
        image_paths = sorted(list(set(image_paths)))
        
        if len(image_paths) == 0:
            raise ValueError(f"ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡: {train_dir}")
        
        # é™åˆ¶æ•°é‡
        if max_images and len(image_paths) > max_images:
            print(f"æ‰¾åˆ° {len(image_paths)} å¼ å›¾ç‰‡ï¼Œå°†ä½¿ç”¨å‰ {max_images} å¼ è¿›è¡Œè®­ç»ƒ")
            image_paths = image_paths[:max_images]
        else:
            print(f"æ‰¾åˆ° {len(image_paths)} å¼ å›¾ç‰‡ï¼Œå°†å…¨éƒ¨ç”¨äºè®­ç»ƒ")
        
        return self.train_from_images(image_paths, aggregation=aggregation)
    
    def match_single_image(self, test_image_path, threshold=0.7):
        """
        ä½¿ç”¨å•å¼ æµ‹è¯•å›¾ç‰‡ä¸è®­ç»ƒç‰¹å¾è¿›è¡ŒåŒ¹é…
        
        Args:
            test_image_path: æµ‹è¯•å›¾ç‰‡è·¯å¾„
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        
        Returns:
            dict: {
                'is_match': æ˜¯å¦åŒ¹é…,
                'similarity': ç›¸ä¼¼åº¦åˆ†æ•°,
                'max_similarity': æœ€é«˜ç›¸ä¼¼åº¦ï¼ˆä»…å½“ä½¿ç”¨'all'èšåˆæ—¶æœ‰æ„ä¹‰ï¼‰,
                'min_similarity': æœ€ä½ç›¸ä¼¼åº¦,
                'avg_similarity': å¹³å‡ç›¸ä¼¼åº¦
            }
        """
        if self.trained_features is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ train_from_images æˆ– train_from_directory è¿›è¡Œè®­ç»ƒï¼")
        
        if not os.path.exists(test_image_path):
            raise FileNotFoundError(f"æµ‹è¯•å›¾ç‰‡ä¸å­˜åœ¨: {test_image_path}")
        
        # æå–æµ‹è¯•å›¾ç‰‡ç‰¹å¾
        test_features = self.extract_features(test_image_path)
        
        # è®¡ç®—ä¸æ‰€æœ‰è®­ç»ƒç‰¹å¾çš„ç›¸ä¼¼åº¦
        similarities = F.cosine_similarity(test_features, self.trained_features)
        
        # ç»Ÿè®¡ä¿¡æ¯
        max_sim = similarities.max().item()
        min_sim = similarities.min().item()
        avg_sim = similarities.mean().item()
        
        # ä½¿ç”¨æœ€é«˜ç›¸ä¼¼åº¦åˆ¤æ–­æ˜¯å¦åŒ¹é…
        is_match = max_sim >= threshold
        
        return {
            'is_match': is_match,
            'similarity': max_sim,  # ä¸»è¦ç›¸ä¼¼åº¦åˆ†æ•°
            'max_similarity': max_sim,
            'min_similarity': min_sim,
            'avg_similarity': avg_sim
        }
    
    def train_multi_class(self, train_dir, aggregation='mean', max_images_per_class=None):
        """
        å¤šç±»åˆ«è®­ç»ƒï¼šç›®å½•ä¸‹çš„å­ç›®å½•æ˜¯æ ‡ç­¾ï¼Œå­ç›®å½•ä¸‹çš„å›¾ç‰‡æ˜¯æ•°æ®
        
        ç›®å½•ç»“æ„ç¤ºä¾‹:
            train_dir/
            â”œâ”€â”€ äººç‰©A/           <- è¿™æ˜¯æ ‡ç­¾
            â”‚   â”œâ”€â”€ photo1.jpg
            â”‚   â”œâ”€â”€ photo2.jpg
            â”‚   â””â”€â”€ ...
            â”œâ”€â”€ äººç‰©B/           <- è¿™æ˜¯æ ‡ç­¾
            â”‚   â”œâ”€â”€ photo1.jpg
            â”‚   â””â”€â”€ ...
            â””â”€â”€ äººç‰©C/
                â””â”€â”€ ...
        
        Args:
            train_dir: è®­ç»ƒç›®å½•è·¯å¾„
            aggregation: ç‰¹å¾èšåˆæ–¹å¼ ('mean', 'max')
            max_images_per_class: æ¯ä¸ªç±»åˆ«æœ€å¤šä½¿ç”¨çš„å›¾ç‰‡æ•°é‡ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
        
        Returns:
            dict: {ç±»åˆ«å: è®­ç»ƒå›¾ç‰‡æ•°é‡}
        """
        if not os.path.exists(train_dir):
            raise FileNotFoundError(f"è®­ç»ƒç›®å½•ä¸å­˜åœ¨: {train_dir}")
        
        # è·å–æ‰€æœ‰å­ç›®å½•ä½œä¸ºç±»åˆ«
        subdirs = [d for d in os.listdir(train_dir) 
                   if os.path.isdir(os.path.join(train_dir, d))]
        
        if len(subdirs) == 0:
            raise ValueError(f"è®­ç»ƒç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ°å­ç›®å½•ï¼ˆç±»åˆ«ï¼‰: {train_dir}")
        
        print(f"\næ‰¾åˆ° {len(subdirs)} ä¸ªç±»åˆ«: {subdirs}")
        print("-" * 70)
        
        # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
        image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.gif', 
                           '*.JPG', '*.JPEG', '*.PNG']
        
        self.class_features = {}
        self.class_image_counts = {}
        
        for class_name in tqdm(subdirs, desc="è®­ç»ƒç±»åˆ«"):
            class_dir = os.path.join(train_dir, class_name)
            
            # è·å–è¯¥ç±»åˆ«ä¸‹çš„æ‰€æœ‰å›¾ç‰‡
            image_paths = []
            for ext in image_extensions:
                image_paths.extend(glob.glob(os.path.join(class_dir, ext)))
            
            # å»é‡å¹¶æ’åº
            image_paths = sorted(list(set(image_paths)))
            
            if len(image_paths) == 0:
                print(f"\nè­¦å‘Š: ç±»åˆ« '{class_name}' ä¸‹æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡ï¼Œè·³è¿‡")
                continue
            
            # é™åˆ¶æ¯ä¸ªç±»åˆ«çš„å›¾ç‰‡æ•°é‡
            if max_images_per_class and len(image_paths) > max_images_per_class:
                image_paths = image_paths[:max_images_per_class]
            
            # æå–è¯¥ç±»åˆ«æ‰€æœ‰å›¾ç‰‡çš„ç‰¹å¾
            features_list = []
            for img_path in image_paths:
                try:
                    features = self.extract_features(img_path)
                    features_list.append(features)
                except Exception as e:
                    print(f"\nè­¦å‘Š: å¤„ç†å¤±è´¥ {img_path}: {str(e)}")
                    continue
            
            if len(features_list) == 0:
                print(f"\nè­¦å‘Š: ç±»åˆ« '{class_name}' æ²¡æœ‰æˆåŠŸæå–ä»»ä½•ç‰¹å¾ï¼Œè·³è¿‡")
                continue
            
            # èšåˆç‰¹å¾
            all_features = torch.cat(features_list, dim=0)
            if aggregation == 'mean':
                class_feature = F.normalize(all_features.mean(dim=0, keepdim=True), dim=1)
            elif aggregation == 'max':
                class_feature = F.normalize(all_features.max(dim=0, keepdim=True)[0], dim=1)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„èšåˆæ–¹å¼: {aggregation}")
            
            self.class_features[class_name] = class_feature
            self.class_image_counts[class_name] = len(features_list)
        
        # æ‰“å°è®­ç»ƒç»“æœ
        print("\n" + "=" * 70)
        print("è®­ç»ƒå®Œæˆï¼")
        print("=" * 70)
        print(f"æ€»ç±»åˆ«æ•°: {len(self.class_features)}")
        for class_name, count in self.class_image_counts.items():
            print(f"  - {class_name}: {count} å¼ å›¾ç‰‡")
        
        return self.class_image_counts
    
    def classify_image(self, test_image_path, top_k=5):
        """
        å¯¹æµ‹è¯•å›¾ç‰‡è¿›è¡Œåˆ†ç±»ï¼Œæ‰¾å‡ºæœ€åƒå“ªä¸ªç±»åˆ«
        
        Args:
            test_image_path: æµ‹è¯•å›¾ç‰‡è·¯å¾„
            top_k: è¿”å›å‰kä¸ªæœ€ç›¸ä¼¼çš„ç±»åˆ«
        
        Returns:
            list: [(ç±»åˆ«å, ç›¸ä¼¼åº¦), ...] æŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—
        """
        if len(self.class_features) == 0:
            raise ValueError("è¯·å…ˆè°ƒç”¨ train_multi_class è¿›è¡Œå¤šç±»åˆ«è®­ç»ƒï¼")
        
        if not os.path.exists(test_image_path):
            raise FileNotFoundError(f"æµ‹è¯•å›¾ç‰‡ä¸å­˜åœ¨: {test_image_path}")
        
        # æå–æµ‹è¯•å›¾ç‰‡ç‰¹å¾
        test_features = self.extract_features(test_image_path)
        
        # è®¡ç®—ä¸æ‰€æœ‰ç±»åˆ«çš„ç›¸ä¼¼åº¦
        results = []
        for class_name, class_feature in self.class_features.items():
            similarity = F.cosine_similarity(test_features, class_feature).item()
            results.append((class_name, similarity))
        
        # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
        results.sort(key=lambda x: x[1], reverse=True)
        
        # è¿”å›å‰kä¸ª
        return results[:top_k]
    
    def classify_batch(self, test_image_paths, top_k=1):
        """
        æ‰¹é‡åˆ†ç±»å¤šå¼ æµ‹è¯•å›¾ç‰‡
        
        Args:
            test_image_paths: æµ‹è¯•å›¾ç‰‡è·¯å¾„åˆ—è¡¨
            top_k: æ¯å¼ å›¾ç‰‡è¿”å›å‰kä¸ªæœ€ç›¸ä¼¼çš„ç±»åˆ«
        
        Returns:
            list: [
                {
                    'image_path': å›¾ç‰‡è·¯å¾„,
                    'predictions': [(ç±»åˆ«å, ç›¸ä¼¼åº¦), ...]
                },
                ...
            ]
        """
        if len(self.class_features) == 0:
            raise ValueError("è¯·å…ˆè°ƒç”¨ train_multi_class è¿›è¡Œå¤šç±»åˆ«è®­ç»ƒï¼")
        
        results = []
        for img_path in tqdm(test_image_paths, desc="åˆ†ç±»è¿›åº¦"):
            try:
                predictions = self.classify_image(img_path, top_k=top_k)
                results.append({
                    'image_path': img_path,
                    'predictions': predictions
                })
            except Exception as e:
                print(f"\nå¤„ç†å¤±è´¥ {img_path}: {str(e)}")
                results.append({
                    'image_path': img_path,
                    'predictions': []
                })
        
        return results

    def search_in_directory(self, train_image_path, search_dir, max_images=100, threshold=0.7):
        """
        åœ¨ç›®å½•ä¸­æœç´¢ä¸è®­ç»ƒå›¾ç‰‡æœ€ç›¸ä¼¼çš„å›¾ç‰‡
        
        Args:
            train_image_path: è®­ç»ƒå›¾ç‰‡è·¯å¾„
            search_dir: æœç´¢ç›®å½•è·¯å¾„
            max_images: æœ€å¤šå¤„ç†çš„å›¾ç‰‡æ•°é‡ï¼ˆé»˜è®¤100ï¼‰
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆä»…ç”¨äºæ ‡è®°æ˜¯å¦åŒ¹é…ï¼‰
        
        Returns:
            list: [(å›¾ç‰‡è·¯å¾„, ç›¸ä¼¼åº¦åˆ†æ•°, æ˜¯å¦åŒ¹é…), ...] æŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—
        """
        if not os.path.exists(train_image_path):
            raise FileNotFoundError(f"è®­ç»ƒå›¾ç‰‡ä¸å­˜åœ¨: {train_image_path}")
        if not os.path.exists(search_dir):
            raise FileNotFoundError(f"æœç´¢ç›®å½•ä¸å­˜åœ¨: {search_dir}")
        
        # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
        image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.gif', '*.JPG', '*.JPEG', '*.PNG']
        
        # è·å–ç›®å½•ä¸­æ‰€æœ‰å›¾ç‰‡
        image_paths = []
        for ext in image_extensions:
            image_paths.extend(glob.glob(os.path.join(search_dir, ext)))
            image_paths.extend(glob.glob(os.path.join(search_dir, '**', ext), recursive=True))
        
        # å»é‡å¹¶æ’åº
        image_paths = sorted(list(set(image_paths)))
        
        # é™åˆ¶æ•°é‡
        total_found = len(image_paths)
        if total_found > max_images:
            image_paths = image_paths[:max_images]
            print(f"æ‰¾åˆ° {total_found} å¼ å›¾ç‰‡ï¼Œå°†å¤„ç†å‰ {max_images} å¼ ")
        else:
            print(f"æ‰¾åˆ° {total_found} å¼ å›¾ç‰‡ï¼Œå°†å…¨éƒ¨å¤„ç†")
        
        if len(image_paths) == 0:
            print("ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶ï¼")
            return []
        
        # æå–è®­ç»ƒå›¾ç‰‡ç‰¹å¾
        print(f"\næ­£åœ¨æå–è®­ç»ƒå›¾ç‰‡ç‰¹å¾: {train_image_path}")
        train_features = self.extract_features(train_image_path)
        
        # æ‰¹é‡å¤„ç†å›¾ç‰‡
        print(f"\næ­£åœ¨å¤„ç† {len(image_paths)} å¼ å›¾ç‰‡...")
        results = []
        
        for img_path in tqdm(image_paths, desc="å¤„ç†è¿›åº¦"):
            try:
                test_features = self.extract_features(img_path)
                similarity = F.cosine_similarity(train_features, test_features).item()
                is_match = similarity >= threshold
                results.append((img_path, similarity, is_match))
            except Exception as e:
                print(f"\nå¤„ç†å›¾ç‰‡å¤±è´¥ {img_path}: {str(e)}")
                continue
        
        # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
        results.sort(key=lambda x: x[1], reverse=True)
        
        return results

def demo_multi_class_classification():
    """
    å¤šç±»åˆ«è®­ç»ƒ + åˆ†ç±»åŒ¹é… çš„ä½¿ç”¨ç¤ºä¾‹
    
    ç›®å½•ç»“æ„:
        train_dir/
        â”œâ”€â”€ äººç‰©A/           <- å­ç›®å½•åå°±æ˜¯æ ‡ç­¾
        â”‚   â”œâ”€â”€ photo1.jpg
        â”‚   â””â”€â”€ photo2.jpg
        â”œâ”€â”€ äººç‰©B/
        â”‚   â””â”€â”€ photo1.jpg
        â””â”€â”€ äººç‰©C/
            â””â”€â”€ ...
    
    ç„¶åç”¨ä¸€å¼ æµ‹è¯•å›¾ç‰‡ï¼Œæ‰¾å‡ºå®ƒæœ€åƒå“ªä¸ªç±»åˆ«ï¼ˆäººç‰©ï¼‰
    """
    print("=" * 70)
    print("å¤šç±»åˆ«è®­ç»ƒ + åˆ†ç±»åŒ¹é… æ¨¡å¼")
    print("=" * 70)
    
    # åˆ›å»ºè¯†åˆ«å™¨
    recognizer = CLIPOneShotRecognizer(model_name='ViT-L/14')
    
    # ========== é…ç½®å‚æ•° ==========
    # è®­ç»ƒç›®å½•ï¼ˆå­ç›®å½•æ˜¯æ ‡ç­¾ï¼Œå­ç›®å½•ä¸‹çš„å›¾ç‰‡æ˜¯æ•°æ®ï¼‰
    train_directory = "classmate_photo_processed/"  # ä¿®æ”¹ä¸ºä½ çš„è®­ç»ƒç›®å½•
    
    # æµ‹è¯•å›¾ç‰‡è·¯å¾„ï¼ˆè¦åˆ†ç±»çš„å•å¼ å›¾ç‰‡ï¼‰
    test_image_path = "frame_261436_id_0028_n_0006.jpg"  # ä¿®æ”¹ä¸ºä½ çš„æµ‹è¯•å›¾ç‰‡
    
    # ç‰¹å¾èšåˆæ–¹å¼ï¼š'mean'ï¼ˆå¹³å‡ï¼Œæ¨èï¼‰, 'max'ï¼ˆæœ€å¤§ï¼‰
    aggregation = 'mean'
    
    # æ˜¾ç¤ºå‰å‡ ä¸ªæœ€ç›¸ä¼¼çš„ç±»åˆ«
    top_k = 5
    
    print(f"\nè®­ç»ƒç›®å½•: {train_directory}")
    print(f"æµ‹è¯•å›¾ç‰‡: {test_image_path}")
    print(f"èšåˆæ–¹å¼: {aggregation}")
    print("-" * 70)
    
    try:
        # ========== æ­¥éª¤1: å¤šç±»åˆ«è®­ç»ƒ ==========
        print("\nã€æ­¥éª¤1ã€‘å¤šç±»åˆ«è®­ç»ƒ...")
        class_counts = recognizer.train_multi_class(
            train_dir=train_directory,
            aggregation=aggregation,
            max_images_per_class=50  # æ¯ä¸ªç±»åˆ«æœ€å¤šä½¿ç”¨50å¼ å›¾ç‰‡
        )
        
        # ========== æ­¥éª¤2: åˆ†ç±»æµ‹è¯•å›¾ç‰‡ ==========
        print(f"\nã€æ­¥éª¤2ã€‘æ­£åœ¨åˆ†ç±»æµ‹è¯•å›¾ç‰‡: {test_image_path}")
        predictions = recognizer.classify_image(
            test_image_path=test_image_path,
            top_k=top_k
        )
        
        # æ˜¾ç¤ºç»“æœ
        print("\n" + "=" * 70)
        print("ğŸ¯ åˆ†ç±»ç»“æœï¼ˆæŒ‰ç›¸ä¼¼åº¦é™åºï¼‰")
        print("=" * 70)
        
        for i, (class_name, similarity) in enumerate(predictions, 1):
            bar_len = int(similarity * 30)  # ç›¸ä¼¼åº¦å¯è§†åŒ–æ¡
            bar = "â–ˆ" * bar_len + "â–‘" * (30 - bar_len)
            print(f"{i}. {class_name}")
            print(f"   ç›¸ä¼¼åº¦: {similarity:.4f} [{bar}]")
            print()
        
        # æœ€ç»ˆé¢„æµ‹
        best_class, best_score = predictions[0]
        print("=" * 70)
        print(f"ğŸ† æœ€ç»ˆé¢„æµ‹: {best_class}")
        print(f"   ç½®ä¿¡åº¦: {best_score:.4f}")
        print("=" * 70)
        
    except FileNotFoundError as e:
        print(f"\nâŒ é”™è¯¯: {e}")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "=" * 70)
    print("ä½¿ç”¨è¯´æ˜ï¼ˆå¤šç±»åˆ«åˆ†ç±»æ¨¡å¼ï¼‰:")
    print("=" * 70)
    print("""
    1. å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆç›®å½•ç»“æ„ï¼‰ï¼š
       train_dir/
       â”œâ”€â”€ ç±»åˆ«A/        <- å­ç›®å½•å = æ ‡ç­¾å
       â”‚   â”œâ”€â”€ img1.jpg  <- è¯¥ç±»åˆ«çš„è®­ç»ƒå›¾ç‰‡
       â”‚   â””â”€â”€ img2.jpg
       â”œâ”€â”€ ç±»åˆ«B/
       â”‚   â””â”€â”€ ...
       â””â”€â”€ ç±»åˆ«C/
           â””â”€â”€ ...
    
    2. è°ƒç”¨æ–¹æ³•ï¼š
       - train_multi_class(train_dir): å¤šç±»åˆ«è®­ç»ƒ
       - classify_image(test_path): åˆ†ç±»å•å¼ å›¾ç‰‡
       - classify_batch(test_paths): æ‰¹é‡åˆ†ç±»å¤šå¼ å›¾ç‰‡
    
    3. é€‚ç”¨åœºæ™¯ï¼š
       - äººè„¸è¯†åˆ«ï¼ˆæ¯ä¸ªäººä¸€ä¸ªæ–‡ä»¶å¤¹ï¼‰
       - ç‰©ä½“åˆ†ç±»ï¼ˆæ¯ç±»ç‰©ä½“ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼‰
       - å›¾åƒæ£€ç´¢ï¼ˆæ‰¾å‡ºæœ€ç›¸ä¼¼çš„ç±»åˆ«ï¼‰
    """)


# ä¸»å…¥å£
if __name__ == "__main__":
    import sys
    
    print("=" * 70)
    print("åŸºäºCLIPçš„å›¾åƒè¯†åˆ«ç³»ç»Ÿ")
    print("=" * 70)
    print("""
    å¯ç”¨æ¨¡å¼:
    1. å¤šç±»åˆ«åˆ†ç±»æ¨¡å¼ (multi_class) - ç›®å½•ä¸‹çš„å­ç›®å½•æ˜¯æ ‡ç­¾ï¼Œæµ‹è¯•å›¾ç‰‡åŒ¹é…æœ€åƒçš„ç±»åˆ«
    2. å•æ ·æœ¬æœç´¢æ¨¡å¼ (search) - ç”¨ä¸€å¼ å›¾ç‰‡åœ¨ç›®å½•ä¸­æœç´¢ç›¸ä¼¼å›¾ç‰‡
    
    ä½¿ç”¨æ–¹æ³•:
        python one_shot_recognition_clip.py              # é»˜è®¤ä½¿ç”¨å¤šç±»åˆ«åˆ†ç±»æ¨¡å¼
        python one_shot_recognition_clip.py multi_class  # å¤šç±»åˆ«åˆ†ç±»æ¨¡å¼
        python one_shot_recognition_clip.py search       # å•æ ·æœ¬æœç´¢æ¨¡å¼
    """)
    
    # é»˜è®¤ä½¿ç”¨å¤šç±»åˆ«åˆ†ç±»æ¨¡å¼
    mode = "multi_class"
    
    # ä»å‘½ä»¤è¡Œå‚æ•°è·å–æ¨¡å¼ï¼ˆå¯é€‰ï¼‰
    if len(sys.argv) > 1:
        mode = sys.argv[1]
    
    if mode == "multi_class":
        # è¿è¡Œå¤šç±»åˆ«åˆ†ç±»ç¤ºä¾‹
        demo_multi_class_classification()
    else:
        print(f"æœªçŸ¥æ¨¡å¼: {mode}")
        print("è¯·ä½¿ç”¨: python one_shot_recognition_clip.py [multi_class|search]")

