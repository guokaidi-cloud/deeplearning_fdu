#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸“é—¨çš„YOLOv8äººè„¸æ£€æµ‹å™¨
ä½¿ç”¨ä¸“ä¸šçš„yolov8-faceæ¨¡å‹è¿›è¡Œé«˜ç²¾åº¦äººè„¸æ£€æµ‹
æ”¯æŒäººè„¸åŒ¹é…è¯†åˆ«åŠŸèƒ½
"""

import cv2
import numpy as np
from pathlib import Path
from ultralytics import YOLO
import argparse
import time
import sys
import os
from collections import defaultdict

# äººè„¸è¯†åˆ«ç›¸å…³å¯¼å…¥
try:
    import face_recognition
    FACE_RECOGNITION_AVAILABLE = True
except ImportError:
    FACE_RECOGNITION_AVAILABLE = False
    print("âš ï¸  face_recognition åº“æœªå®‰è£…ï¼Œäººè„¸è¯†åˆ«åŠŸèƒ½ä¸å¯ç”¨")
    print("   å®‰è£…å‘½ä»¤: pip install face_recognition")

# ä¸­æ–‡å­—ä½“æ”¯æŒ
try:
    from PIL import Image, ImageDraw, ImageFont
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    print("âš ï¸  PIL åº“æœªå®‰è£…ï¼Œä¸­æ–‡å§“åæ˜¾ç¤ºåŠŸèƒ½ä¸å¯ç”¨")
    print("   å®‰è£…å‘½ä»¤: pip install Pillow")

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥å…¶ä»–è„šæœ¬
sys.path.append(str(Path(__file__).parent))

from face_detector import YOLOv8FaceDetector


# ======================== äººè„¸è¯†åˆ«é…ç½® ========================
# ä¸­æ–‡å­—ä½“è·¯å¾„é…ç½®ï¼ˆæ ¹æ®ç³»ç»Ÿè°ƒæ•´ï¼‰
CHINESE_FONT_PATHS = [
    "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",  # Linux (Ubuntu/Debian)
    "/usr/share/fonts/truetype/droid/DroidSansFallbackFull.ttf",  # Linux å¤‡é€‰
    "/usr/share/fonts/wqy-microhei/wqy-microhei.ttc",  # Linux æ–‡æ³‰é©¿
    "/usr/share/fonts/truetype/wqy/wqy-zenhei.ttc",  # Linux æ–‡æ³‰é©¿æ­£é»‘
    "C:/Windows/Fonts/simhei.ttf",  # Windows é»‘ä½“
    "C:/Windows/Fonts/msyh.ttc",  # Windows å¾®è½¯é›…é»‘
    "/System/Library/Fonts/PingFang.ttc",  # macOS è‹¹æ–¹
    "/System/Library/Fonts/STHeiti Light.ttc",  # macOS åæ–‡é»‘ä½“
]

# é»˜è®¤äººè„¸åŒ¹é…å®¹å·®
DEFAULT_FACE_TOLERANCE = 0.6


def get_chinese_font(font_size=20):
    """
    è·å–å¯ç”¨çš„ä¸­æ–‡å­—ä½“
    
    Args:
        font_size (int): å­—ä½“å¤§å°
        
    Returns:
        ImageFont: å­—ä½“å¯¹è±¡ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸­æ–‡å­—ä½“åˆ™è¿”å›é»˜è®¤å­—ä½“
    """
    if not PIL_AVAILABLE:
        return None
    
    for font_path in CHINESE_FONT_PATHS:
        if Path(font_path).exists():
            try:
                return ImageFont.truetype(font_path, font_size)
            except IOError:
                continue
    
    print("âš ï¸  æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨é»˜è®¤å­—ä½“ï¼ˆä¸­æ–‡å¯èƒ½æ˜¾ç¤ºä¸ºæ–¹å—ï¼‰")
    return ImageFont.load_default()


def build_student_database(photo_folder, verbose=True):
    """
    æ„å»ºå­¦ç”Ÿäººè„¸ç‰¹å¾æ•°æ®åº“
    
    Args:
        photo_folder (str): å­¦ç”Ÿç…§ç‰‡æ–‡ä»¶å¤¹è·¯å¾„
        verbose (bool): æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        
    Returns:
        dict: å­¦ç”Ÿå§“ååˆ°äººè„¸ç‰¹å¾å‘é‡çš„æ˜ å°„
    """
    if not FACE_RECOGNITION_AVAILABLE:
        print("âŒ face_recognition åº“æœªå®‰è£…ï¼Œæ— æ³•æ„å»ºäººè„¸æ•°æ®åº“")
        return {}
    
    student_db = {}
    photo_folder = Path(photo_folder)
    
    if not photo_folder.exists():
        print(f"âŒ ç…§ç‰‡æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {photo_folder}")
        return {}
    
    for filename in os.listdir(photo_folder):
        if filename.lower().endswith((".jpg", ".jpeg", ".png")):
            name = os.path.splitext(filename)[0]
            photo_path = photo_folder / filename
            try:
                image = face_recognition.load_image_file(str(photo_path))
                face_encodings = face_recognition.face_encodings(image)
                if face_encodings:
                    student_db[name] = face_encodings[0]
                    if verbose:
                        print(f"âœ… æˆåŠŸåŠ è½½ {name} çš„äººè„¸ç‰¹å¾")
                else:
                    if verbose:
                        print(f"âš ï¸ æœªåœ¨ {filename} ä¸­æ£€æµ‹åˆ°äººè„¸ï¼Œå·²è·³è¿‡")
            except Exception as e:
                if verbose:
                    print(f"âŒ å¤„ç† {filename} å¤±è´¥: {str(e)}")
    
    print(f"\nğŸ“Š äººè„¸æ•°æ®åº“æ„å»ºå®Œæˆï¼Œå…±åŠ è½½ {len(student_db)} åå­¦ç”Ÿçš„ç‰¹å¾\n")
    return student_db


def match_face(face_encoding, student_db, tolerance=DEFAULT_FACE_TOLERANCE):
    """
    å°†äººè„¸ç‰¹å¾ä¸æ•°æ®åº“è¿›è¡ŒåŒ¹é…
    
    Args:
        face_encoding: å¾…åŒ¹é…çš„äººè„¸ç‰¹å¾å‘é‡
        student_db (dict): å­¦ç”Ÿäººè„¸ç‰¹å¾æ•°æ®åº“
        tolerance (float): åŒ¹é…å®¹å·®ï¼Œè¶Šå°è¶Šä¸¥æ ¼
        
    Returns:
        tuple: (åŒ¹é…çš„å§“å, åŒ¹é…è·ç¦»)ï¼Œå¦‚æœæœªåŒ¹é…åˆ™è¿”å› ("æœªçŸ¥äººå‘˜", None)
    """
    if not FACE_RECOGNITION_AVAILABLE or not student_db:
        return "æœªçŸ¥äººå‘˜", None
    
    known_face_encodings = list(student_db.values())
    known_face_names = list(student_db.keys())
    
    if len(known_face_encodings) == 0:
        return "æœªçŸ¥äººå‘˜", None
    
    # è®¡ç®—ä¸æ‰€æœ‰å·²çŸ¥äººè„¸çš„è·ç¦»
    face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)
    best_match_index = np.argmin(face_distances)
    best_distance = face_distances[best_match_index]
    
    # æ£€æŸ¥æ˜¯å¦åŒ¹é…
    matches = face_recognition.compare_faces(known_face_encodings, face_encoding, tolerance=tolerance)
    if matches[best_match_index]:
        return known_face_names[best_match_index], best_distance
    
    return "æœªçŸ¥äººå‘˜", best_distance


def check_and_download_model(model_path, model_name='yolov8n-face'):
    """
    æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è‡ªåŠ¨ä¸‹è½½
    
    Args:
        model_path (Path): æ¨¡å‹æ–‡ä»¶è·¯å¾„
        model_name (str): æ¨¡å‹åç§°
        
    Returns:
        bool: æ¨¡å‹æ˜¯å¦å¯ç”¨
    """
    if model_path.exists():
        print(f"âœ… æ‰¾åˆ°äººè„¸æ£€æµ‹æ¨¡å‹: {model_path}")
        return True
    
    print(f"âš ï¸  æœªæ‰¾åˆ°äººè„¸æ£€æµ‹æ¨¡å‹: {model_path}")
    print(f"ğŸ”„ å¼€å§‹è‡ªåŠ¨ä¸‹è½½ {model_name} æ¨¡å‹...")
    
    try:
        # å¯¼å…¥ä¸‹è½½è„šæœ¬
        from scripts.download_face_models import download_file
        
        # äººè„¸æ£€æµ‹æ¨¡å‹ä¸‹è½½é“¾æ¥
        face_model_urls = {
            'yolov8n-face': 'https://github.com/lindevs/yolov8-face/releases/latest/download/yolov8n-face-lindevs.pt',
            'yolov12l-face': 'https://github.com/akanametov/yolov8-face/releases/download/v0.0.0/yolov12l-face.pt',
        }
        
        if model_name not in face_model_urls:
            print(f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")
            return False
        
        # åˆ›å»ºæ¨¡å‹ç›®å½•
        model_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ä¸‹è½½æ¨¡å‹
        download_file(face_model_urls[model_name], str(model_path))
        
        if model_path.exists() and model_path.stat().st_size > 1024 * 1024:
            print(f"âœ… æ¨¡å‹ä¸‹è½½æˆåŠŸ: {model_path}")
            return True
        else:
            print(f"âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥æˆ–æ–‡ä»¶æŸå")
            return False
            
    except Exception as e:
        print(f"âŒ è‡ªåŠ¨ä¸‹è½½å¤±è´¥: {e}")
        print(f"ğŸ’¡ è¯·æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°: {model_path}")
        if model_name == 'yolov8n-face':
            print(f"   ä¸‹è½½é“¾æ¥: https://github.com/lindevs/yolov8-face/releases/latest/download/yolov8n-face-lindevs.pt")
        else:
            print(f"   ä¸‹è½½é“¾æ¥: https://huggingface.co/Bingsu/adetailer/tree/main")
        return False


class YOLOv8SpecializedFaceDetector(YOLOv8FaceDetector):
    """ä¸“é—¨çš„YOLOv8äººè„¸æ£€æµ‹å™¨ï¼Œä½¿ç”¨ä¼˜åŒ–çš„äººè„¸æ£€æµ‹æ¨¡å‹ï¼Œæ”¯æŒäººè„¸è¯†åˆ«å’ŒByteTrackè·Ÿè¸ª"""
    
    def __init__(self, model_name='yolov8n-face', conf_threshold=0.3, device='auto', 
                 models_dir='models', model_path=None, student_photos_folder=None, face_tolerance=DEFAULT_FACE_TOLERANCE,
                 enable_tracking=False, tracker_type='bytetrack', track_buffer=30):
        """
        åˆå§‹åŒ–ä¸“é—¨çš„äººè„¸æ£€æµ‹å™¨
        
        Args:
            model_name (str): æ¨¡å‹åç§° ('yolov8n-face', 'yolov8s-face')
            conf_threshold (float): ç½®ä¿¡åº¦é˜ˆå€¼
            device (str): è¿è¡Œè®¾å¤‡
            models_dir (str): æ¨¡å‹ç›®å½•
            model_path (str|None): è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„ï¼ˆä¼˜å…ˆäº model_name/models_dirï¼‰
            student_photos_folder (str): å­¦ç”Ÿç…§ç‰‡æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆç”¨äºäººè„¸è¯†åˆ«ï¼‰
            face_tolerance (float): äººè„¸åŒ¹é…å®¹å·®
            enable_tracking (bool): æ˜¯å¦å¯ç”¨è·Ÿè¸ª
            tracker_type (str): è·Ÿè¸ªå™¨ç±»å‹ ('bytetrack' æˆ– 'botsort')
            track_buffer (int): è·Ÿè¸ªç¼“å†²å¸§æ•°ï¼ˆè½¨è¿¹æœ€å¤§ä¸¢å¤±å¸§æ•°ï¼‰
        """
        self.model_name = model_name
        self.models_dir = Path(models_dir)
        self.face_tolerance = face_tolerance
        self.student_db = {}
        self.chinese_font = None
        self.enable_tracking = enable_tracking
        self.tracker_type = tracker_type
        self.track_buffer = track_buffer
        
        # æ„é€ æ¨¡å‹è·¯å¾„ï¼Œä¼˜å…ˆä½¿ç”¨è‡ªå®šä¹‰è·¯å¾„
        if model_path:
            model_path = Path(model_path)
            custom_model = True
        else:
            model_path = self.models_dir / f"{model_name}.pt"
            custom_model = False
        
        # æ£€æŸ¥å¹¶ä¸‹è½½/éªŒè¯æ¨¡å‹
        if not model_path.exists():
            if custom_model:
                raise RuntimeError(f"è‡ªå®šä¹‰æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            if not check_and_download_model(model_path, model_name):
                raise RuntimeError(f"æ— æ³•è·å–äººè„¸æ£€æµ‹æ¨¡å‹: {model_name}")
        
        # ä½¿ç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(
            model_path=str(model_path),
            conf_threshold=conf_threshold,
            device=device
        )
        
        # åŠ è½½å­¦ç”Ÿäººè„¸æ•°æ®åº“
        if student_photos_folder:
            self.load_student_database(student_photos_folder)
        
        # åŠ è½½ä¸­æ–‡å­—ä½“
        if PIL_AVAILABLE:
            self.chinese_font = get_chinese_font(font_size=20)
        
        print(f"ğŸ¯ ä¸“ä¸šäººè„¸æ£€æµ‹å™¨å·²å°±ç»ª")
        print(f"ğŸ“¦ æ¨¡å‹: {model_name}")
        print(f"ğŸšï¸  ç½®ä¿¡åº¦é˜ˆå€¼: {conf_threshold}")
        if enable_tracking:
            print(f"ğŸ”„ è·Ÿè¸ªå™¨: {tracker_type.upper()} (buffer={track_buffer})")
        else:
            print(f"ğŸ”„ è·Ÿè¸ª: å·²ç¦ç”¨")
        if self.student_db:
            print(f"ğŸ‘¥ äººè„¸è¯†åˆ«: å·²åŠ è½½ {len(self.student_db)} åå­¦ç”Ÿ")
    
    def load_student_database(self, photo_folder):
        """
        åŠ è½½å­¦ç”Ÿäººè„¸æ•°æ®åº“
        
        Args:
            photo_folder (str): å­¦ç”Ÿç…§ç‰‡æ–‡ä»¶å¤¹è·¯å¾„
        """
        self.student_db = build_student_database(photo_folder, verbose=True)
        return len(self.student_db)
    
    def recognize_face_with_bbox(self, full_image, bbox):
        """
        ä½¿ç”¨YOLOæ£€æµ‹åˆ°çš„è¾¹ç•Œæ¡†ç›´æ¥åœ¨åŸå›¾ä¸Šè¯†åˆ«äººè„¸
        
        Args:
            full_image: å®Œæ•´å›¾åƒ (BGRæ ¼å¼)
            bbox: YOLOæ£€æµ‹åˆ°çš„è¾¹ç•Œæ¡† [x1, y1, x2, y2]
            
        Returns:
            tuple: (å§“å, åŒ¹é…è·ç¦»)
        """
        if not FACE_RECOGNITION_AVAILABLE or not self.student_db:
            return "æœªçŸ¥äººå‘˜", None
        
        # è½¬æ¢ä¸ºRGB
        if len(full_image.shape) == 3 and full_image.shape[2] == 3:
            rgb_image = cv2.cvtColor(full_image, cv2.COLOR_BGR2RGB)
        else:
            rgb_image = full_image
        
        # å°†YOLOçš„ [x1, y1, x2, y2] è½¬æ¢ä¸º face_recognition çš„ (top, right, bottom, left) æ ¼å¼
        x1, y1, x2, y2 = bbox
        face_location = (y1, x2, y2, x1)  # (top, right, bottom, left)
        
        # æå–äººè„¸ç‰¹å¾ï¼ˆä½¿ç”¨YOLOæ£€æµ‹åˆ°çš„ä½ç½®ï¼Œè·³è¿‡face_recognitionçš„äººè„¸æ£€æµ‹ï¼‰
        try:
            face_encodings = face_recognition.face_encodings(rgb_image, known_face_locations=[face_location])
            if face_encodings:
                return match_face(face_encodings[0], self.student_db, self.face_tolerance)
        except Exception as e:
            print(f"âš ï¸  äººè„¸è¯†åˆ«å¤±è´¥: {e}")
        
        return "æœªçŸ¥äººå‘˜", None
    
    def draw_chinese_text(self, image, text, position, font_color=(255, 255, 255), bg_color=(0, 0, 0)):
        """
        åœ¨å›¾åƒä¸Šç»˜åˆ¶ä¸­æ–‡æ–‡æœ¬
        
        Args:
            image: OpenCVå›¾åƒ (BGR)
            text: è¦ç»˜åˆ¶çš„æ–‡æœ¬
            position: æ–‡æœ¬ä½ç½® (x, y)
            font_color: å­—ä½“é¢œè‰² (R, G, B)
            bg_color: èƒŒæ™¯é¢œè‰² (R, G, B)
            
        Returns:
            å¤„ç†åçš„å›¾åƒ
        """
        if not PIL_AVAILABLE or self.chinese_font is None:
            # å¦‚æœPILä¸å¯ç”¨ï¼Œä½¿ç”¨OpenCVç»˜åˆ¶ï¼ˆä¸­æ–‡ä¼šæ˜¾ç¤ºä¸ºæ–¹å—ï¼‰
            cv2.putText(image, text, position, cv2.FONT_HERSHEY_SIMPLEX, 0.6, font_color[::-1], 2)
            return image
        
        # è½¬æ¢ä¸ºPILå›¾åƒ
        pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(pil_image)
        
        x, y = position
        
        # è·å–æ–‡æœ¬è¾¹ç•Œæ¡†
        bbox = draw.textbbox((0, 0), text, font=self.chinese_font)
        text_width = bbox[2] - bbox[0]
        text_height = bbox[3] - bbox[1]
        
        # ç»˜åˆ¶èƒŒæ™¯çŸ©å½¢
        padding = 5
        draw.rectangle(
            [(x, y), (x + text_width + padding * 2, y + text_height + padding * 2)],
            fill=bg_color
        )
        
        # ç»˜åˆ¶æ–‡æœ¬
        draw.text((x + padding, y + padding), text, font=self.chinese_font, fill=font_color)
        
        # è½¬å›OpenCVæ ¼å¼
        return cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
    
    def detect_and_track(self, image, recognize=True, persist=True):
        """
        ä½¿ç”¨YOLOå†…ç½®çš„ByteTrack/BotSORTè¿›è¡Œæ£€æµ‹å’Œè·Ÿè¸ª
        
        Args:
            image: è¾“å…¥å›¾åƒ
            recognize (bool): æ˜¯å¦è¿›è¡Œäººè„¸è¯†åˆ«
            persist (bool): æ˜¯å¦æŒä¹…åŒ–è·Ÿè¸ªIDï¼ˆè·¨å¸§ä¿æŒIDï¼‰
            
        Returns:
            list: è·Ÿè¸ªç»“æœåˆ—è¡¨ï¼ŒåŒ…å«track_id
        """
        if isinstance(image, np.ndarray):
            original_image = image.copy()
            original_shape = image.shape[:2]
        else:
            original_image = np.array(image)
            original_shape = original_image.shape[:2]
        
        # ä½¿ç”¨YOLOçš„trackæ–¹æ³•è¿›è¡Œè·Ÿè¸ª
        results = self.model.track(
            original_image, 
            conf=self.conf_threshold,
            persist=persist,
            tracker=f"{self.tracker_type}.yaml",
            verbose=False
        )
        
        tracked_faces = []
        
        for result in results:
            boxes = result.boxes
            if boxes is not None and len(boxes) > 0:
                for box in boxes:
                    # è·å–è¾¹ç•Œæ¡†åæ ‡
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    confidence = float(box.conf[0].cpu().numpy())
                    
                    # è·å–è·Ÿè¸ªID
                    track_id = None
                    if box.id is not None:
                        track_id = int(box.id[0].cpu().numpy())
                    
                    # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
                    x1_int = max(0, int(x1))
                    y1_int = max(0, int(y1))
                    x2_int = min(original_shape[1], int(x2))
                    y2_int = min(original_shape[0], int(y2))
                    
                    face_info = {
                        'bbox': [x1_int, y1_int, x2_int, y2_int],
                        'confidence': confidence,
                        'track_id': track_id,
                        'name': "æœªçŸ¥äººå‘˜",
                        'match_distance': None
                    }
                    
                    # è¿›è¡Œäººè„¸è¯†åˆ«
                    if recognize and self.student_db and FACE_RECOGNITION_AVAILABLE:
                        face_width = x2_int - x1_int
                        face_height = y2_int - y1_int
                        if face_width > 20 and face_height > 20:
                            name, distance = self.recognize_face_with_bbox(
                                original_image, 
                                [x1_int, y1_int, x2_int, y2_int]
                            )
                            face_info['name'] = name
                            face_info['match_distance'] = distance
                    
                    tracked_faces.append(face_info)
        
        return tracked_faces
    
    def detect_faces(self, image, visualize=True, recognize=True):
        """
        æ£€æµ‹å›¾ç‰‡ä¸­çš„äººè„¸ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒäººè„¸è¯†åˆ«ï¼‰
        
        Args:
            image: è¾“å…¥å›¾åƒ
            visualize (bool): æ˜¯å¦å¯è§†åŒ–æ£€æµ‹ç»“æœ
            recognize (bool): æ˜¯å¦è¿›è¡Œäººè„¸è¯†åˆ«
            
        Returns:
            tuple: (æ£€æµ‹ç»“æœ, å¯è§†åŒ–å›¾åƒ)
        """
        # ä¿å­˜åŸå§‹å›¾åƒå°ºå¯¸
        if isinstance(image, np.ndarray):
            original_image = image.copy()
            original_shape = image.shape[:2]  # (height, width)
        else:
            original_image = np.array(image)
            original_shape = original_image.shape[:2]
        
        # è¿è¡Œæ¨ç†
        results = self.model(original_image, conf=self.conf_threshold, verbose=False)
        
        faces = []
        # ç¡®ä¿ä½¿ç”¨åŸå§‹åˆ†è¾¨ç‡çš„å›¾åƒè¿›è¡Œå¯è§†åŒ–
        vis_image = original_image.copy()
        
        # è§£ææ£€æµ‹ç»“æœ
        for result in results:
            boxes = result.boxes
            if boxes is not None:
                for box in boxes:
                    # è·å–è¾¹ç•Œæ¡†åæ ‡
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    confidence = box.conf[0].cpu().numpy()
                    
                    # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
                    x1_int = max(0, int(x1))
                    y1_int = max(0, int(y1))
                    x2_int = min(original_shape[1], int(x2))
                    y2_int = min(original_shape[0], int(y2))
                    
                    face_info = {
                        'bbox': [x1_int, y1_int, x2_int, y2_int],
                        'confidence': float(confidence),
                        'name': "æœªçŸ¥äººå‘˜",
                        'match_distance': None
                    }
                    
                    # è¿›è¡Œäººè„¸è¯†åˆ«ï¼ˆä½¿ç”¨YOLOæ£€æµ‹åˆ°çš„è¾¹ç•Œæ¡†ä½ç½®ï¼‰
                    if recognize and self.student_db and FACE_RECOGNITION_AVAILABLE:
                        # ç¡®ä¿äººè„¸åŒºåŸŸè¶³å¤Ÿå¤§
                        face_width = x2_int - x1_int
                        face_height = y2_int - y1_int
                        if face_width > 20 and face_height > 20:
                            # ä½¿ç”¨YOLOçš„è¾¹ç•Œæ¡†ç›´æ¥åœ¨åŸå›¾ä¸Šæå–ç‰¹å¾
                            name, distance = self.recognize_face_with_bbox(
                                original_image, 
                                [x1_int, y1_int, x2_int, y2_int]
                            )
                            face_info['name'] = name
                            face_info['match_distance'] = distance
                    
                    faces.append(face_info)
                    
                    if visualize:
                        name = face_info['name']
                        is_known = name != "æœªçŸ¥äººå‘˜"
                        
                        # æ ¹æ®æ˜¯å¦è¯†åˆ«æˆåŠŸé€‰æ‹©é¢œè‰²
                        box_color = (0, 255, 0) if is_known else (0, 255, 255)  # ç»¿è‰²=å·²è¯†åˆ«, é»„è‰²=æœªè¯†åˆ«
                        
                        # ç»˜åˆ¶è¾¹ç•Œæ¡†
                        cv2.rectangle(vis_image, (x1_int, y1_int), (x2_int, y2_int), box_color, 2)
                        
                        # æ„å»ºæ ‡ç­¾æ–‡æœ¬
                        if is_known:
                            label = f'{name} ({confidence:.2f})'
                        else:
                            label = f'Face: {confidence:.3f}'
                        
                        # ç»˜åˆ¶æ ‡ç­¾ï¼ˆæ”¯æŒä¸­æ–‡ï¼‰
                        label_y = max(0, y1_int - 5)
                        
                        if is_known and PIL_AVAILABLE and self.chinese_font:
                            # ä½¿ç”¨ä¸­æ–‡å­—ä½“ç»˜åˆ¶å§“å
                            vis_image = self.draw_chinese_text(
                                vis_image, 
                                name, 
                                (x1_int, label_y - 25),
                                font_color=(255, 255, 255),
                                bg_color=(0, 128, 0)
                            )
                        else:
                            # ä½¿ç”¨OpenCVç»˜åˆ¶
                            font_scale = 0.4
                            thickness = 1
                            padding = 4
                            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)[0]
                            cv2.rectangle(vis_image, (x1_int, y1_int - label_size[1] - padding), 
                                        (x1_int + label_size[0], y1_int), box_color, -1)
                            cv2.putText(vis_image, label, (x1_int, y1_int - padding // 2), 
                                      cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 0, 0), thickness)
        
        return faces, vis_image


def process_video_with_yolov8(detector, video_path, output_path=None, show_video=False, 
                              max_frames=None, start_time=None, end_time=None, save_faces=True,
                              save_interval_sec=5.0, enable_recognition=True, enable_tracking=True):
    """
    ä½¿ç”¨YOLOv8å¤„ç†è§†é¢‘æ–‡ä»¶è¿›è¡Œäººè„¸æ£€æµ‹ã€è¯†åˆ«å’Œè·Ÿè¸ª
    
    Args:
        detector: YOLOv8äººè„¸æ£€æµ‹å™¨å®ä¾‹
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
        show_video: æ˜¯å¦æ˜¾ç¤ºè§†é¢‘
        max_frames: æœ€å¤§å¤„ç†å¸§æ•°
        start_time: å¼€å§‹æ—¶é—´
        end_time: ç»“æŸæ—¶é—´
        save_faces (bool): æ˜¯å¦ä¿å­˜è£å‰ªçš„äººè„¸åˆ°dataç›®å½•
        save_interval_sec (float): ä¿å­˜äººè„¸çš„æ—¶é—´é—´éš”ï¼ˆç§’ï¼‰ï¼Œç”¨äºé™é¢‘ä¿å­˜
        enable_recognition (bool): æ˜¯å¦å¯ç”¨äººè„¸è¯†åˆ«
        enable_tracking (bool): æ˜¯å¦å¯ç”¨è·Ÿè¸ª (ByteTrack/BotSORT)
    """
    print(f"ğŸ¥ å¼€å§‹å¤„ç†è§†é¢‘: {video_path}")
    if enable_recognition and hasattr(detector, 'student_db') and detector.student_db:
        print(f"ğŸ‘¥ äººè„¸è¯†åˆ«: å·²å¯ç”¨ï¼Œæ•°æ®åº“ä¸­æœ‰ {len(detector.student_db)} äºº")
    
    # æ£€æŸ¥è·Ÿè¸ªåŠŸèƒ½
    tracking_enabled = enable_tracking and hasattr(detector, 'enable_tracking') and detector.enable_tracking
    if tracking_enabled:
        tracker_type = getattr(detector, 'tracker_type', 'bytetrack')
        print(f"ğŸ”„ è·Ÿè¸ª: å·²å¯ç”¨ ({tracker_type.upper()})")
    else:
        print(f"ğŸ”„ è·Ÿè¸ª: å·²ç¦ç”¨")
    
    # åˆ›å»ºdataç›®å½•ç”¨äºä¿å­˜äººè„¸
    if save_faces:
        data_dir = Path(video_path).parent / 'data'
        data_dir.mkdir(parents=True, exist_ok=True)
        print(f"ğŸ“ äººè„¸å°†ä¿å­˜åˆ°: {data_dir}")

    # ä»…æ£€æµ‹ä¸ä¿å­˜ï¼Œä¸åšè·Ÿè¸ª
    
    # æ‰“å¼€è§†é¢‘æ–‡ä»¶
    cap = cv2.VideoCapture(str(video_path))
    if not cap.isOpened():
        print(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
        return
    
    # è·å–è§†é¢‘å±æ€§
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps if fps > 0 else 0
    
    print(f"ğŸ“Š è§†é¢‘ä¿¡æ¯: {width}x{height}, {fps}FPS, æ€»å¸§æ•°={total_frames}, æ—¶é•¿={duration:.1f}ç§’")
    
    # è§£ææ—¶é—´å‚æ•°ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œä¸opencv_face_detectorä¿æŒä¸€è‡´ï¼‰
    start_frame = 0
    end_frame = total_frames
    
    if start_time:
        try:
            if ':' in start_time:
                parts = start_time.split(':')
                if len(parts) == 3:  # HH:MM:SS
                    hours, minutes, seconds = map(float, parts)
                    start_seconds = hours * 3600 + minutes * 60 + seconds
                elif len(parts) == 2:  # MM:SS
                    minutes, seconds = map(float, parts)
                    start_seconds = minutes * 60 + seconds
            else:
                start_seconds = float(start_time)
            
            start_frame = int(start_seconds * fps)
            start_frame = max(0, min(start_frame, total_frames - 1))
            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
            print(f"â© è·³è½¬åˆ°å¼€å§‹æ—¶é—´: {start_seconds:.1f}ç§’ (ç¬¬{start_frame}å¸§)")
        except:
            print(f"âš ï¸  æ— æ•ˆçš„å¼€å§‹æ—¶é—´æ ¼å¼: {start_time}")
    
    if end_time:
        try:
            if ':' in end_time:
                parts = end_time.split(':')
                if len(parts) == 3:  # HH:MM:SS
                    hours, minutes, seconds = map(float, parts)
                    end_seconds = hours * 3600 + minutes * 60 + seconds
                elif len(parts) == 2:  # MM:SS
                    minutes, seconds = map(float, parts)
                    end_seconds = minutes * 60 + seconds
            else:
                end_seconds = float(end_time)
                
            end_frame = int(end_seconds * fps)
            end_frame = max(start_frame, min(end_frame, total_frames))
            print(f"â¹ï¸  ç»“æŸæ—¶é—´: {end_seconds:.1f}ç§’ (ç¬¬{end_frame}å¸§)")
        except:
            print(f"âš ï¸  æ— æ•ˆçš„ç»“æŸæ—¶é—´æ ¼å¼: {end_time}")
    
    # è®¡ç®—å¤„ç†å¸§æ•°
    process_frames = end_frame - start_frame
    if max_frames:
        process_frames = min(process_frames, max_frames)
    
    # è®¾ç½®è§†é¢‘å†™å…¥å™¨
    def _create_writer(path: Path, fps: int, size):
        """å°è¯•å¤šç§ç¼–ç ï¼Œæå‡æµè§ˆå™¨å¯æ’­æ”¾æ€§ï¼Œå¹¶ç»™å‡ºæ—¥å¿—"""
        width, height = size
        codec_candidates = [
            ("avc1", "H.264 (æµè§ˆå™¨å…¼å®¹æ€§å¥½ï¼Œéœ€ç³»ç»Ÿæ”¯æŒ)"),
            ("mp4v", "MPEG-4 Part 2 (å…¼å®¹æ€§ä¸€èˆ¬)"),
            ("XVID", "XVID (å¤‡ç”¨)"),
        ]
        for fourcc_tag, desc in codec_candidates:
            fourcc = cv2.VideoWriter_fourcc(*fourcc_tag)
            writer = cv2.VideoWriter(str(path), fourcc, fps, (width, height))
            if writer.isOpened():
                print(f"âœ… ä½¿ç”¨ç¼–ç  {fourcc_tag} - {desc}")
                return writer, fourcc_tag
            else:
                print(f"âš ï¸ åˆ›å»ºå†™å…¥å™¨å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªç¼–ç : {fourcc_tag}")
        return None, None

    writer = None
    if output_path:
        writer, used_codec = _create_writer(output_path, fps, (width, height))
        if writer is None:
            print("âŒ æ— æ³•åˆ›å»ºä»»ä½•å¯ç”¨çš„è§†é¢‘å†™å…¥å™¨ï¼Œåœæ­¢å¤„ç†")
            return
        print(f"ğŸ“ è¾“å‡ºè§†é¢‘: {output_path}")
        print(f"ğŸ“ è¾“å‡ºåˆ†è¾¨ç‡: {width}x{height}, FPS: {fps}, ç¼–ç : {used_codec}")
    
    # å¤„ç†ç»Ÿè®¡
    processed_frames = 0
    total_faces = 0
    process_start_time = time.time()
    last_save_time = -1e9  # æ§åˆ¶ä¿å­˜é¢‘ç‡çš„æ—¶é—´æˆ³
    track_save_counts = defaultdict(int)  # ç”¨äºæŒ‰track_idä¿å­˜è®¡æ•°
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # è·å–å½“å‰å¸§ä½ç½®
            current_frame = int(cap.get(cv2.CAP_PROP_POS_FRAMES))
            current_time_sec = current_frame / fps if fps > 0 else 0
            
            # æ£€æŸ¥å¤„ç†é™åˆ¶
            if processed_frames >= process_frames:
                break
                
            if max_frames and processed_frames >= max_frames:
                print(f"â¹ï¸  å·²è¾¾åˆ°æœ€å¤§å¤„ç†å¸§æ•°: {max_frames}")
                break
            
            # æ£€æµ‹äººè„¸ï¼ˆæ ¹æ®æ˜¯å¦å¯ç”¨è·Ÿè¸ªé€‰æ‹©ä¸åŒæ–¹æ³•ï¼‰
            if tracking_enabled:
                # ä½¿ç”¨YOLOå†…ç½®çš„ByteTrack/BotSORTè·Ÿè¸ª
                faces = detector.detect_and_track(frame, recognize=enable_recognition, persist=True)
            else:
                # ä»…æ£€æµ‹ï¼Œä¸è·Ÿè¸ª
                faces, _ = detector.detect_faces(frame, visualize=False, recognize=enable_recognition)
            
            total_faces += len(faces)
            
            # ç»Ÿè®¡è¯†åˆ«ç»“æœ
            recognized_names = [f['name'] for f in faces if f.get('name') and f['name'] != "æœªçŸ¥äººå‘˜"]
            
            # è‡ªå®šä¹‰å¯è§†åŒ–ï¼ˆæ”¯æŒè·Ÿè¸ªIDæ˜¾ç¤ºï¼‰
            vis_frame = frame.copy()
            for face in faces:
                x1, y1, x2, y2 = face['bbox']
                confidence = face['confidence']
                name = face.get('name', 'æœªçŸ¥äººå‘˜')
                track_id = face.get('track_id', None)
                is_known = name != "æœªçŸ¥äººå‘˜"
                
                # æ ¹æ®æ˜¯å¦è¯†åˆ«æˆåŠŸé€‰æ‹©é¢œè‰²
                if tracking_enabled and track_id is not None:
                    # è·Ÿè¸ªæ¨¡å¼ï¼šä½¿ç”¨track_idç”Ÿæˆé¢œè‰²
                    color_hash = hash(str(track_id)) % 0xFFFFFF
                    box_color = ((color_hash >> 16) & 0xFF, (color_hash >> 8) & 0xFF, color_hash & 0xFF)
                    # ç¡®ä¿é¢œè‰²è¶³å¤Ÿäº®
                    box_color = tuple(max(c, 50) for c in box_color)
                else:
                    box_color = (0, 255, 0) if is_known else (0, 255, 255)  # ç»¿è‰²=å·²è¯†åˆ«, é»„è‰²=æœªè¯†åˆ«
                
                # ç»˜åˆ¶è¾¹ç•Œæ¡†
                cv2.rectangle(vis_frame, (x1, y1), (x2, y2), box_color, 2)
                
                # æ„å»ºæ ‡ç­¾æ–‡æœ¬ï¼ˆé¿å…ä¸­æ–‡æ˜¾ç¤ºä¸ºé—®å·ï¼‰
                if tracking_enabled and track_id is not None:
                    # è·Ÿè¸ªæ¨¡å¼ï¼šæ˜¾ç¤ºIDå’Œç½®ä¿¡åº¦
                    label = f'ID:{track_id} ({confidence:.2f})'
                else:
                    # éè·Ÿè¸ªæ¨¡å¼ï¼šåªæ˜¾ç¤ºç½®ä¿¡åº¦
                    label = f'Face ({confidence:.2f})'
                
                # ç»˜åˆ¶æ ‡ç­¾
                label_y = max(0, y1 - 5)
                font_scale = 0.5
                thickness = 1
                padding = 4
                label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)[0]
                cv2.rectangle(vis_frame, (x1, y1 - label_size[1] - padding * 2), 
                            (x1 + label_size[0] + padding, y1), box_color, -1)
                cv2.putText(vis_frame, label, (x1 + padding // 2, y1 - padding), 
                          cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 0, 0), thickness)

            # ç¡®ä¿vis_frameçš„åˆ†è¾¨ç‡ä¸åŸå§‹frameä¸€è‡´
            if vis_frame.shape[:2] != frame.shape[:2]:
                print(f"âš ï¸  è­¦å‘Š: vis_frameåˆ†è¾¨ç‡ {vis_frame.shape[:2]} ä¸åŸå§‹frameåˆ†è¾¨ç‡ {frame.shape[:2]} ä¸ä¸€è‡´ï¼Œä½¿ç”¨åŸå§‹frame")
                vis_frame = frame.copy()
                # é‡æ–°ç»˜åˆ¶æ£€æµ‹æ¡†
                for face in faces:
                    x1, y1, x2, y2 = face['bbox']
                    confidence = face['confidence']
                    cv2.rectangle(vis_frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 255), 2)
                    label = f'Face: {confidence:.3f}'
                    font_scale = 0.4  # å­—ä½“ç¼©æ”¾å› å­ï¼ˆ0.4æ¯”åŸæ¥çš„0.6æ›´å°ï¼‰
                    thickness = 1     # çº¿æ¡ç²—ç»†ï¼ˆ1æ¯”åŸæ¥çš„2æ›´ç»†ï¼‰
                    padding = 4      # paddingï¼ˆ4æ¯”åŸæ¥çš„10æ›´å°ï¼‰
                    label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)[0]
                    cv2.rectangle(vis_frame, (int(x1), int(y1) - label_size[1] - padding), 
                                (int(x1) + label_size[0], int(y1)), (0, 255, 255), -1)
                    cv2.putText(vis_frame, label, (int(x1), int(y1) - padding // 2), 
                              cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 0, 0), thickness)
            
            # ä¿å­˜è£å‰ªçš„äººè„¸
            if save_faces:
                # æŒ‰æ—¶é—´é—´éš”é™é¢‘ä¿å­˜ï¼›è‹¥æœ¬å¸§æœªåˆ°è¾¾ä¿å­˜é—´éš”åˆ™è·³è¿‡ä¿å­˜
                allow_save = (current_time_sec - last_save_time) >= save_interval_sec
                if allow_save:
                    last_save_time = current_time_sec
                    for face_idx, face in enumerate(faces):
                        x1, y1, x2, y2 = face['bbox']
                        confidence = face.get('confidence', 0.0)
                        track_id = face.get('track_id', None)
                        name = face.get('name', 'æœªçŸ¥äººå‘˜')
                        
                        # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
                        x1 = max(0, int(x1))
                        y1 = max(0, int(y1))
                        x2 = min(frame.shape[1], int(x2))
                        y2 = min(frame.shape[0], int(y2))
                        
                        # è£å‰ªäººè„¸åŒºåŸŸ
                        face_crop = frame[y1:y2, x1:x2]
                        
                        # åªä¿å­˜æœ‰æ•ˆçš„äººè„¸ï¼ˆå°ºå¯¸ä¸èƒ½å¤ªå°ï¼‰
                        if face_crop.shape[0] > 20 and face_crop.shape[1] > 20:
                            # æ ¹æ®æ˜¯å¦æœ‰track_idå†³å®šä¿å­˜è·¯å¾„
                            if track_id is not None:
                                # æœ‰track_idï¼šæŒ‰IDåˆ†ç›®å½•ä¿å­˜
                                if name != "æœªçŸ¥äººå‘˜":
                                    id_dir = data_dir / f"id_{int(track_id):04d}_{name}"
                                else:
                                    id_dir = data_dir / f"id_{int(track_id):04d}"
                                id_dir.mkdir(parents=True, exist_ok=True)
                                track_save_counts[track_id] += 1
                                face_filename = f"frame_{current_frame:06d}_id_{int(track_id):04d}_n_{track_save_counts[track_id]:04d}.jpg"
                                face_path = id_dir / face_filename
                            else:
                                # æ— track_idï¼šæŒ‰å¸§å·å’Œäººè„¸ç´¢å¼•ä¿å­˜
                                face_filename = f"frame_{current_frame:06d}_face_{face_idx:02d}_conf_{confidence:.3f}.jpg"
                                face_path = data_dir / face_filename
                            cv2.imwrite(str(face_path), face_crop)
            
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            elapsed_time = time.time() - process_start_time
            current_fps = processed_frames / elapsed_time if elapsed_time > 0 else 0
            
            hours = int(current_time_sec // 3600)
            minutes = int((current_time_sec % 3600) // 60)
            seconds = int(current_time_sec % 60)
            
            stats_text = [
                f'Time: {hours:02d}:{minutes:02d}:{seconds:02d} (Frame: {current_frame})',
                f'Progress: {processed_frames+1}/{process_frames}',
                f'Current Faces: {len(faces)}',
                f'Recognized: {len(recognized_names)}',
                f'Processing FPS: {current_fps:.1f}'
            ]
            
            # å¦‚æœæœ‰è¯†åˆ«åˆ°çš„äººï¼Œæ˜¾ç¤ºå§“å
            if recognized_names:
                names_str = ', '.join(recognized_names[:3])  # æœ€å¤šæ˜¾ç¤º3ä¸ªåå­—
                if len(recognized_names) > 3:
                    names_str += f'... (+{len(recognized_names)-3})'
                stats_text.append(f'Names: {names_str}')
            
            for i, text in enumerate(stats_text):
                y_pos = 30 + i * 25
                cv2.putText(vis_frame, text, (10, y_pos), 
                          cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
            
            # ä¿å­˜å¸§
            if writer:
                # ç¡®ä¿vis_frameçš„åˆ†è¾¨ç‡ä¸VideoWriterè®¾ç½®çš„åˆ†è¾¨ç‡ä¸€è‡´
                if vis_frame.shape[:2] != (height, width):
                    vis_frame = cv2.resize(vis_frame, (width, height), interpolation=cv2.INTER_LINEAR)
                writer.write(vis_frame)
            
            # æ˜¾ç¤ºè§†é¢‘
            if show_video:
                cv2.imshow('YOLOv8äººè„¸æ£€æµ‹', vis_frame)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    print("ğŸ‘¤ ç”¨æˆ·æŒ‰'q'é”®é€€å‡º")
                    break
            
            processed_frames += 1
            
            # å®šæœŸè¾“å‡ºè¿›åº¦
            if processed_frames % 100 == 0:
                progress = (processed_frames / process_frames) * 100
                current_time_str = f"{hours:02d}:{minutes:02d}:{seconds:02d}"
                print(f"ğŸ“ˆ å¤„ç†è¿›åº¦: {processed_frames}/{process_frames} "
                      f"({progress:.1f}%) | æ—¶é—´: {current_time_str} | æ£€æµ‹åˆ°äººè„¸: {total_faces}")
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­å¤„ç†")
    
    finally:
        # æ¸…ç†èµ„æº
        cap.release()
        if writer:
            writer.release()
        if show_video:
            cv2.destroyAllWindows()
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    elapsed_time = time.time() - process_start_time
    avg_fps = processed_frames / elapsed_time if elapsed_time > 0 else 0
    
    print(f"\nğŸ“Š å¤„ç†å®Œæˆç»Ÿè®¡:")
    print(f"   â±ï¸  å¤„ç†æ—¶é—´: {elapsed_time:.1f}ç§’")
    print(f"   ğŸ“¹ å¤„ç†å¸§æ•°: {processed_frames}")
    print(f"   ğŸ¯ æ£€æµ‹äººè„¸: {total_faces}")
    print(f"   âš¡ å¹³å‡FPS: {avg_fps:.1f}")
    print(f"   ğŸ“ å¹³å‡æ¯å¸§äººè„¸æ•°: {total_faces/processed_frames:.1f}" if processed_frames > 0 else "")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='YOLOv8ä¸“ä¸šäººè„¸æ£€æµ‹å™¨')
    parser.add_argument('--input', type=str, required=True,
                       help='è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, 
                       help='è¾“å‡ºè§†é¢‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--show', action='store_true',
                       help='æ˜¾ç¤ºæ£€æµ‹è¿‡ç¨‹')
    parser.add_argument('--max-frames', type=int,
                       help='æœ€å¤§å¤„ç†å¸§æ•° (ç”¨äºæµ‹è¯•)')
    parser.add_argument('--start-time', type=str,
                       help='å¼€å§‹æ—¶é—´ (ç§’æ•°æˆ– HH:MM:SS æ ¼å¼)')
    parser.add_argument('--end-time', type=str, 
                       help='ç»“æŸæ—¶é—´ (ç§’æ•°æˆ– HH:MM:SS æ ¼å¼)')
    parser.add_argument('--model', type=str, default='yolov8n-face',
                       choices=['yolov8n-face', 'yolov12l-face'],
                       help='äººè„¸æ£€æµ‹æ¨¡å‹åç§°')
    parser.add_argument('--model-path', type=str, default=None,
                       help='è‡ªå®šä¹‰æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼ˆä¼˜å…ˆä½¿ç”¨è¯¥è·¯å¾„ï¼Œå­˜åœ¨åˆ™ä¸å†ä¸‹è½½ï¼‰')
    parser.add_argument('--conf', type=float, default=0.3, 
                       help='ç½®ä¿¡åº¦é˜ˆå€¼')
    parser.add_argument('--device', type=str, default='auto', 
                       help='è¿è¡Œè®¾å¤‡')
    parser.add_argument('--models-dir', type=str, default='models',
                       help='æ¨¡å‹å­˜æ”¾ç›®å½•')
    parser.add_argument('--save-faces', action='store_true', default=False,
                       help='ä¿å­˜è£å‰ªçš„äººè„¸åˆ°åŸå§‹æ•°æ®çš„dataç›®å½•')
    parser.add_argument('--no-save-faces', dest='save_faces', action='store_false',
                       help='ä¸ä¿å­˜è£å‰ªçš„äººè„¸')
    parser.add_argument('--save-interval-sec', type=float, default=5.0,
                       help='ä¿å­˜äººè„¸çš„æ—¶é—´é—´éš”ï¼ˆç§’ï¼‰ï¼Œç”¨äºé™é¢‘ä¿å­˜ï¼Œé»˜è®¤5ç§’')
    parser.add_argument('--student-photos', type=str, default=None,
                       help='å­¦ç”Ÿç…§ç‰‡æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆç”¨äºäººè„¸è¯†åˆ«ï¼‰')
    parser.add_argument('--face-tolerance', type=float, default=DEFAULT_FACE_TOLERANCE,
                       help=f'äººè„¸åŒ¹é…å®¹å·®ï¼Œè¶Šå°è¶Šä¸¥æ ¼ï¼Œé»˜è®¤{DEFAULT_FACE_TOLERANCE}')
    parser.add_argument('--no-recognition', action='store_true',
                       help='ç¦ç”¨äººè„¸è¯†åˆ«åŠŸèƒ½')
    
    # è·Ÿè¸ªç›¸å…³å‚æ•° (ä½¿ç”¨YOLOå†…ç½®çš„ByteTrack/BotSORT)
    parser.add_argument('--track', action='store_true', default=False,
                       help='å¯ç”¨è·Ÿè¸ªåŠŸèƒ½ (ByteTrack/BotSORT)')
    parser.add_argument('--no-track', dest='track', action='store_false',
                       help='ç¦ç”¨è·Ÿè¸ªåŠŸèƒ½')
    parser.add_argument('--tracker', type=str, default='bytetrack',
                       choices=['bytetrack', 'botsort'],
                       help='è·Ÿè¸ªå™¨ç±»å‹: bytetrack(é»˜è®¤,å¿«é€Ÿ) æˆ– botsort(æ›´ç²¾ç¡®)')
    parser.add_argument('--track-buffer', type=int, default=30,
                       help='è·Ÿè¸ªç¼“å†²å¸§æ•°ï¼ˆè½¨è¿¹æœ€å¤§ä¸¢å¤±å¸§æ•°ï¼‰ï¼Œé»˜è®¤30')
    
    args = parser.parse_args()
    
    try:
        # åˆå§‹åŒ–ä¸“ä¸šäººè„¸æ£€æµ‹å™¨
        print(f"ğŸš€ åˆå§‹åŒ–YOLOv8ä¸“ä¸šäººè„¸æ£€æµ‹å™¨...")
        detector = YOLOv8SpecializedFaceDetector(
            model_name=args.model,
            conf_threshold=args.conf,
            device=args.device,
            models_dir=args.models_dir,
            model_path=args.model_path,
            student_photos_folder=args.student_photos,
            face_tolerance=args.face_tolerance,
            enable_tracking=args.track,
            tracker_type=args.tracker,
            track_buffer=args.track_buffer
        )
        
        enable_recognition = not args.no_recognition and len(detector.student_db) > 0
        enable_tracking = args.track
        
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
        input_path = Path(args.input)
        if not input_path.exists():
            print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
            return
        
        # è®¾ç½®è¾“å‡ºè·¯å¾„
        output_path = args.output
        if not output_path:
            output_path = input_path.parent / f"yolov8_detected_{input_path.name}"
        
        # å¤„ç†è§†é¢‘æ–‡ä»¶
        if input_path.is_file() and input_path.suffix.lower() in ['.mp4', '.avi', '.mov', '.mkv']:
            # åˆ›å»ºä¸“ç”¨çš„è§†é¢‘å¤„ç†æ–¹æ³•
            process_video_with_yolov8(
                detector=detector,
                video_path=input_path,
                output_path=output_path,
                show_video=args.show,
                max_frames=args.max_frames,
                start_time=args.start_time,
                end_time=args.end_time,
                save_faces=args.save_faces,
                save_interval_sec=args.save_interval_sec,
                enable_recognition=enable_recognition,
                enable_tracking=enable_tracking
            )
        
        # å¤„ç†å›¾ç‰‡æ–‡ä»¶
        elif input_path.is_file() and input_path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']:
            image = cv2.imread(str(input_path))
            faces, vis_image = detector.detect_faces(image, visualize=True, recognize=enable_recognition)
            
            # ä¿å­˜è£å‰ªçš„äººè„¸
            if args.save_faces and len(faces) > 0:
                data_dir = input_path.parent / 'data'
                data_dir.mkdir(parents=True, exist_ok=True)
                print(f"ğŸ“ äººè„¸å°†ä¿å­˜åˆ°: {data_dir}")
                
                for face_idx, face in enumerate(faces):
                    x1, y1, x2, y2 = face['bbox']
                    confidence = face['confidence']
                    
                    # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
                    x1 = max(0, int(x1))
                    y1 = max(0, int(y1))
                    x2 = min(image.shape[1], int(x2))
                    y2 = min(image.shape[0], int(y2))
                    
                    # è£å‰ªäººè„¸åŒºåŸŸ
                    face_crop = image[y1:y2, x1:x2]
                    
                    # åªä¿å­˜æœ‰æ•ˆçš„äººè„¸ï¼ˆå°ºå¯¸ä¸èƒ½å¤ªå°ï¼‰
                    if face_crop.shape[0] > 20 and face_crop.shape[1] > 20:
                        face_filename = f"{input_path.stem}_face_{face_idx:02d}_conf_{confidence:.3f}.jpg"
                        face_path = data_dir / face_filename
                        cv2.imwrite(str(face_path), face_crop)
                        print(f"   ğŸ’¾ ä¿å­˜äººè„¸: {face_filename}")
            
            # ä¿å­˜ç»“æœ
            if not output_path:
                output_path = input_path.parent / f"yolov8_detected_{input_path.name}"
            
            cv2.imwrite(str(output_path), vis_image)
            
            print(f"âœ… æ£€æµ‹åˆ° {len(faces)} ä¸ªäººè„¸")
            recognized_count = 0
            for i, face in enumerate(faces):
                bbox = face['bbox']
                conf = face['confidence']
                name = face.get('name', 'æœªçŸ¥äººå‘˜')
                if name != 'æœªçŸ¥äººå‘˜':
                    recognized_count += 1
                    print(f"   äººè„¸{i+1}: {name}, åæ ‡{bbox}, ç½®ä¿¡åº¦{conf:.3f}")
                else:
                    print(f"   äººè„¸{i+1}: æœªçŸ¥äººå‘˜, åæ ‡{bbox}, ç½®ä¿¡åº¦{conf:.3f}")
            
            if enable_recognition:
                print(f"ğŸ“Š è¯†åˆ«ç»“æœ: {recognized_count}/{len(faces)} äººè¢«è¯†åˆ«")
            
            print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
            
            # æ˜¾ç¤ºç»“æœ
            if args.show:
                cv2.imshow('YOLOv8ä¸“ä¸šäººè„¸æ£€æµ‹', vis_image)
                cv2.waitKey(0)
                cv2.destroyAllWindows()
        
        else:
            print(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {input_path}")
    
    except Exception as e:
        print(f"âŒ æ£€æµ‹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("ğŸ’¡ å»ºè®®:")
        print("   1. æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
        print("   2. ç¡®è®¤ç½‘ç»œè¿æ¥æ­£å¸¸ï¼ˆç”¨äºä¸‹è½½æ¨¡å‹ï¼‰")
        print("   3. æ£€æŸ¥è®¾å¤‡å’ŒCUDAç¯å¢ƒ")


if __name__ == '__main__':
    main()
