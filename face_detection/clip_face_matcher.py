#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäº CLIP çš„äººè„¸åŒ¹é…å™¨ - ä½¿ç”¨ç›¸ä¼¼åº¦åŒ¹é…æ‰¾åˆ°æœ€åƒçš„äºº
æ— éœ€é¢„è®­ç»ƒ SVMï¼Œç›´æ¥ç”¨ CLIP ç‰¹å¾çš„ä½™å¼¦ç›¸ä¼¼åº¦è¿›è¡ŒåŒ¹é…
"""

from __future__ import annotations

import os
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
import torch
import torch.nn.functional as F
from PIL import Image

try:
    import clip
    CLIP_AVAILABLE = True
except ImportError:
    CLIP_AVAILABLE = False
    print("âš ï¸  clip åº“æœªå®‰è£…ï¼ŒCLIPäººè„¸åŒ¹é…åŠŸèƒ½ä¸å¯ç”¨")
    print("   å®‰è£…å‘½ä»¤: pip install git+https://github.com/openai/CLIP.git")


@dataclass
class MatchResult:
    """åŒ¹é…ç»“æœ"""
    name: str                    # åŒ¹é…çš„äººå
    similarity: float            # ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
    all_similarities: Optional[Dict[str, float]] = None  # æ‰€æœ‰äººçš„ç›¸ä¼¼åº¦


class ClipFaceMatcher:
    """
    åŸºäº CLIP çš„äººè„¸åŒ¹é…å™¨
    
    ä½¿ç”¨æ–¹å¼:
        1. åˆå§‹åŒ–æ—¶åŠ è½½ç…§ç‰‡åº“: matcher = ClipFaceMatcher(photo_folder="photos/")
        2. æˆ–æ‰‹åŠ¨åŠ è½½: matcher.load_photo_database("photos/")
        3. åŒ¹é…äººè„¸: result = matcher.match(face_image)
    """

    def __init__(
        self,
        photo_folder: Optional[str] = None,
        threshold: float = 0.65,
        clip_model_name: str = "ViT-B/32",
        device: str = "auto",
    ):
        """
        åˆå§‹åŒ– CLIP äººè„¸åŒ¹é…å™¨
        
        Args:
            photo_folder: äººè„¸ç…§ç‰‡åº“æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆæ–‡ä»¶åä½œä¸ºäººåï¼‰
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼è¿”å›"æœªçŸ¥äººå‘˜"
            clip_model_name: CLIP æ¨¡å‹åç§° (ViT-B/32, ViT-B/16, ViT-L/14)
            device: è¿è¡Œè®¾å¤‡ (auto/cuda/cpu)
        """
        if not CLIP_AVAILABLE:
            raise RuntimeError("clip åº“æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨ CLIP äººè„¸åŒ¹é…")
        
        self.threshold = threshold
        self.clip_model_name = clip_model_name
        
        if device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device

        # åŠ è½½ CLIP æ¨¡å‹
        print(f"ğŸ”„ åŠ è½½ CLIP æ¨¡å‹: {clip_model_name} (è®¾å¤‡: {self.device})")
        self.clip_model, self.preprocess = clip.load(self.clip_model_name, device=self.device)
        self.clip_model.eval()
        print(f"âœ… CLIP æ¨¡å‹åŠ è½½å®Œæˆ")

        # äººè„¸ç‰¹å¾æ•°æ®åº“: {äººå: ç‰¹å¾å‘é‡}
        self.face_database: Dict[str, np.ndarray] = {}
        
        # å¦‚æœæä¾›äº†ç…§ç‰‡æ–‡ä»¶å¤¹ï¼Œè‡ªåŠ¨åŠ è½½
        if photo_folder:
            self.load_photo_database(photo_folder)

    def _extract_embedding(self, image: Image.Image) -> np.ndarray:
        """ä» PIL å›¾åƒæå– CLIP ç‰¹å¾å‘é‡"""
        tensor = self.preprocess(image.convert("RGB")).unsqueeze(0).to(self.device)
        with torch.no_grad():
            feat = self.clip_model.encode_image(tensor)
            feat = F.normalize(feat, dim=1)  # å½’ä¸€åŒ–
        return feat.cpu().numpy()[0]

    def _extract_embedding_from_bgr(self, image_bgr: np.ndarray) -> np.ndarray:
        """ä» BGR å›¾åƒï¼ˆOpenCV æ ¼å¼ï¼‰æå– CLIP ç‰¹å¾å‘é‡"""
        # BGR -> RGB -> PIL Image
        image = Image.fromarray(image_bgr[:, :, ::-1])
        return self._extract_embedding(image)

    def load_photo_database(self, photo_folder: str, verbose: bool = True) -> int:
        """
        åŠ è½½äººè„¸ç…§ç‰‡åº“
        
        Args:
            photo_folder: ç…§ç‰‡æ–‡ä»¶å¤¹è·¯å¾„ï¼Œæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰ä½œä¸ºäººå
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
            
        Returns:
            åŠ è½½çš„äººæ•°
        """
        photo_folder = Path(photo_folder)
        if not photo_folder.exists():
            print(f"âŒ ç…§ç‰‡æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {photo_folder}")
            return 0

        self.face_database.clear()
        valid_extensions = {".jpg", ".jpeg", ".png", ".bmp"}
        
        if verbose:
            print(f"ğŸ“‚ åŠ è½½äººè„¸ç…§ç‰‡åº“: {photo_folder}")
        
        for img_file in sorted(photo_folder.iterdir()):
            if img_file.suffix.lower() not in valid_extensions:
                continue
            
            name = img_file.stem  # æ–‡ä»¶åä½œä¸ºäººå
            try:
                image = Image.open(img_file).convert("RGB")
                embedding = self._extract_embedding(image)
                self.face_database[name] = embedding
                if verbose:
                    print(f"   âœ… {name}")
            except Exception as e:
                if verbose:
                    print(f"   âŒ {name}: {e}")
        
        print(f"ğŸ“Š äººè„¸åº“åŠ è½½å®Œæˆï¼Œå…± {len(self.face_database)} äºº\n")
        return len(self.face_database)

    def match(self, face_image: np.ndarray, return_all: bool = False) -> MatchResult:
        """
        åŒ¹é…äººè„¸ï¼Œè¿”å›æœ€ç›¸ä¼¼çš„äºº
        
        Args:
            face_image: äººè„¸å›¾åƒ (BGR æ ¼å¼, OpenCV)
            return_all: æ˜¯å¦è¿”å›æ‰€æœ‰äººçš„ç›¸ä¼¼åº¦
            
        Returns:
            MatchResult: åŒ¹é…ç»“æœï¼ŒåŒ…å«äººåå’Œç›¸ä¼¼åº¦
        """
        if not self.face_database:
            return MatchResult(name="æœªçŸ¥äººå‘˜", similarity=0.0)
        
        # æå–äººè„¸ç‰¹å¾
        try:
            face_embedding = self._extract_embedding_from_bgr(face_image)
        except Exception as e:
            print(f"âš ï¸  ç‰¹å¾æå–å¤±è´¥: {e}")
            return MatchResult(name="æœªçŸ¥äººå‘˜", similarity=0.0)
        
        # è®¡ç®—ä¸æ‰€æœ‰äººçš„ç›¸ä¼¼åº¦
        similarities: Dict[str, float] = {}
        for name, db_embedding in self.face_database.items():
            # ä½™å¼¦ç›¸ä¼¼åº¦
            sim = float(np.dot(face_embedding, db_embedding))
            similarities[name] = sim
        
        # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„äºº
        best_name = max(similarities, key=similarities.get)
        best_similarity = similarities[best_name]
        
        # é˜ˆå€¼åˆ¤æ–­
        if self.threshold is not None and best_similarity < self.threshold:
            result_name = "æœªçŸ¥äººå‘˜"
        else:
            result_name = best_name
        
        return MatchResult(
            name=result_name,
            similarity=best_similarity,
            all_similarities=similarities if return_all else None
        )

    def match_pil(self, face_image: Image.Image, return_all: bool = False) -> MatchResult:
        """
        åŒ¹é…äººè„¸ (PIL å›¾åƒç‰ˆæœ¬)
        
        Args:
            face_image: äººè„¸å›¾åƒ (PIL Image)
            return_all: æ˜¯å¦è¿”å›æ‰€æœ‰äººçš„ç›¸ä¼¼åº¦
            
        Returns:
            MatchResult: åŒ¹é…ç»“æœ
        """
        if not self.face_database:
            return MatchResult(name="æœªçŸ¥äººå‘˜", similarity=0.0)
        
        try:
            face_embedding = self._extract_embedding(face_image)
        except Exception as e:
            print(f"âš ï¸  ç‰¹å¾æå–å¤±è´¥: {e}")
            return MatchResult(name="æœªçŸ¥äººå‘˜", similarity=0.0)
        
        similarities: Dict[str, float] = {}
        for name, db_embedding in self.face_database.items():
            sim = float(np.dot(face_embedding, db_embedding))
            similarities[name] = sim
        
        best_name = max(similarities, key=similarities.get)
        best_similarity = similarities[best_name]
        
        if self.threshold is not None and best_similarity < self.threshold:
            result_name = "æœªçŸ¥äººå‘˜"
        else:
            result_name = best_name
        
        return MatchResult(
            name=result_name,
            similarity=best_similarity,
            all_similarities=similarities if return_all else None
        )

    @property
    def num_people(self) -> int:
        """è¿”å›æ•°æ®åº“ä¸­çš„äººæ•°"""
        return len(self.face_database)

    @property
    def names(self) -> List[str]:
        """è¿”å›æ•°æ®åº“ä¸­æ‰€æœ‰äººå"""
        return list(self.face_database.keys())


# æµ‹è¯•ç”¨ä¾‹
if __name__ == "__main__":
    import cv2
    
    print("=" * 60)
    print("CLIP äººè„¸åŒ¹é…å™¨æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•ç…§ç‰‡åº“è·¯å¾„ï¼ˆè¯·ä¿®æ”¹ä¸ºæ‚¨çš„è·¯å¾„ï¼‰
    photo_folder = "../classmate_photo_processed"
    
    if not Path(photo_folder).exists():
        print(f"âŒ è¯·ä¿®æ”¹ photo_folder è·¯å¾„: {photo_folder}")
    else:
        # åˆå§‹åŒ–åŒ¹é…å™¨
        matcher = ClipFaceMatcher(
            photo_folder=photo_folder,
            threshold=0.65,
            clip_model_name="ViT-B/32"
        )
        
        print(f"\nå·²åŠ è½½ {matcher.num_people} äºº")
        print(f"äººå‘˜åˆ—è¡¨: {matcher.names[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ª
