#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
YOLOv8 äººè„¸æ£€æµ‹å™¨
åŸºäº Ultralytics YOLOv8 å®ç°çš„å®æ—¶äººè„¸æ£€æµ‹ç³»ç»Ÿ
"""

import cv2
import numpy as np
from pathlib import Path
import torch
from ultralytics import YOLO
import argparse
import time


class YOLOFaceDetector:
    
    def __init__(self, model_path='models/yolov8n-face.pt', conf_threshold=0.5, device='auto'):
        """
        åˆå§‹åŒ–äººè„¸æ£€æµ‹å™¨
        
        Args:
            model_path (str): æ¨¡å‹æ–‡ä»¶è·¯å¾„
            conf_threshold (float): ç½®ä¿¡åº¦é˜ˆå€¼
            device (str): è¿è¡Œè®¾å¤‡ ('cpu', 'cuda', 'auto')
        """
        self.conf_threshold = conf_threshold
        self.device = device
        
        # åŠ è½½æ¨¡å‹
        try:
            self.model = YOLO(model_path)
            print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_path}")
            print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.model.device}")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def detect_faces(self, image, visualize=True):
        """
        æ£€æµ‹å›¾ç‰‡ä¸­çš„äººè„¸
        
        Args:
            image: è¾“å…¥å›¾åƒ (numpy array æˆ– PIL Image)
            visualize (bool): æ˜¯å¦å¯è§†åŒ–æ£€æµ‹ç»“æœ
            
        Returns:
            tuple: (æ£€æµ‹ç»“æœ, å¯è§†åŒ–å›¾åƒ)
        """
        # è¿è¡Œæ¨ç† - ä¸“é—¨çš„äººè„¸æ£€æµ‹æ¨¡å‹é€šå¸¸åªæ£€æµ‹äººè„¸
        results = self.model(image, conf=self.conf_threshold)
        
        faces = []
        vis_image = image.copy() if isinstance(image, np.ndarray) else np.array(image)
        
        # è§£ææ£€æµ‹ç»“æœ
        for result in results:
            boxes = result.boxes
            if boxes is not None:
                for box in boxes:
                    # è·å–è¾¹ç•Œæ¡†åæ ‡
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    confidence = box.conf[0].cpu().numpy()
                    
                    face_info = {
                        'bbox': [int(x1), int(y1), int(x2), int(y2)],
                        'confidence': float(confidence)
                    }
                    faces.append(face_info)
                    
                    if visualize:
                        # ç»˜åˆ¶è¾¹ç•Œæ¡†
                        cv2.rectangle(vis_image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)
                        
                        # æ·»åŠ ç½®ä¿¡åº¦æ ‡ç­¾
                        label = f'Face: {confidence:.2f}'
                        label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
                        cv2.rectangle(vis_image, (int(x1), int(y1) - label_size[1] - 10), 
                                    (int(x1) + label_size[0], int(y1)), (0, 255, 0), -1)
                        cv2.putText(vis_image, label, (int(x1), int(y1) - 5), 
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)
        
        return faces, vis_image
    
    def detect_video(self, source=0, save_path=None, show=True):
        """
        å®æ—¶è§†é¢‘äººè„¸æ£€æµ‹
        
        Args:
            source: è§†é¢‘æº (0ä¸ºæ‘„åƒå¤´, ä¹Ÿå¯ä»¥æ˜¯è§†é¢‘æ–‡ä»¶è·¯å¾„)
            save_path (str): ä¿å­˜ç»“æœè§†é¢‘çš„è·¯å¾„
            show (bool): æ˜¯å¦æ˜¾ç¤ºæ£€æµ‹ç»“æœ
        """
        cap = cv2.VideoCapture(source)
        
        if not cap.isOpened():
            print(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘æº: {source}")
            return
        
        # è·å–è§†é¢‘å±æ€§
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        # è®¾ç½®è§†é¢‘å†™å…¥å™¨
        writer = None
        if save_path:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            writer = cv2.VideoWriter(save_path, fourcc, fps, (width, height))
        
        print(f"ğŸ¥ å¼€å§‹æ£€æµ‹ - æŒ‰ 'q' é€€å‡º")
        
        frame_count = 0
        start_time = time.time()
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # æ£€æµ‹äººè„¸
                faces, vis_frame = self.detect_faces(frame, visualize=True)
                
                # æ·»åŠ FPSä¿¡æ¯
                frame_count += 1
                elapsed_time = time.time() - start_time
                fps_current = frame_count / elapsed_time
                
                cv2.putText(vis_frame, f'FPS: {fps_current:.1f} | Faces: {len(faces)}', 
                          (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)
                
                # ä¿å­˜ç»“æœ
                if writer:
                    writer.write(vis_frame)
                
                # æ˜¾ç¤ºç»“æœ
                if show:
                    cv2.imshow('YOLOv8 äººè„¸æ£€æµ‹', vis_frame)
                    if cv2.waitKey(1) & 0xFF == ord('q'):
                        break
        
        except KeyboardInterrupt:
            print("\nâ¹ï¸  æ£€æµ‹å·²åœæ­¢")
        
        finally:
            cap.release()
            if writer:
                writer.release()
            cv2.destroyAllWindows()
    
    def batch_detect(self, image_dir, output_dir):
        """
        æ‰¹é‡æ£€æµ‹å›¾ç‰‡ä¸­çš„äººè„¸
        
        Args:
            image_dir (str): è¾“å…¥å›¾ç‰‡ç›®å½•
            output_dir (str): è¾“å‡ºç»“æœç›®å½•
        """
        image_dir = Path(image_dir)
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']
        image_files = []
        for ext in image_extensions:
            image_files.extend(image_dir.glob(f'*{ext}'))
            image_files.extend(image_dir.glob(f'*{ext.upper()}'))
        
        if not image_files:
            print(f"âŒ åœ¨ {image_dir} ä¸­æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
            return
        
        print(f"ğŸ“‚ æ‰¾åˆ° {len(image_files)} ä¸ªå›¾ç‰‡æ–‡ä»¶")
        
        for i, image_file in enumerate(image_files):
            print(f"ğŸ” å¤„ç† ({i+1}/{len(image_files)}): {image_file.name}")
            
            # è¯»å–å›¾ç‰‡
            image = cv2.imread(str(image_file))
            if image is None:
                print(f"âš ï¸  æ— æ³•è¯»å–å›¾ç‰‡: {image_file}")
                continue
            
            # æ£€æµ‹äººè„¸
            faces, vis_image = self.detect_faces(image, visualize=True)
            
            # ä¿å­˜ç»“æœ
            output_file = output_dir / f"detected_{image_file.name}"
            cv2.imwrite(str(output_file), vis_image)
            
            print(f"   âœ… æ£€æµ‹åˆ° {len(faces)} ä¸ªäººè„¸ï¼Œç»“æœå·²ä¿å­˜åˆ° {output_file}")
        
        print(f"ğŸ‰ æ‰¹é‡æ£€æµ‹å®Œæˆï¼ç»“æœä¿å­˜åœ¨ {output_dir}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='YOLOv8 äººè„¸æ£€æµ‹å™¨')
    parser.add_argument('--model', type=str, default='models/yolov8n-face.pt',
                       help='äººè„¸æ£€æµ‹æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--source', type=str, default='0', 
                       help='è¾“å…¥æº (æ‘„åƒå¤´ID/è§†é¢‘æ–‡ä»¶/å›¾ç‰‡æ–‡ä»¶/å›¾ç‰‡ç›®å½•)')
    parser.add_argument('--output', type=str, default='runs/detect', 
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--conf', type=float, default=0.5, 
                       help='ç½®ä¿¡åº¦é˜ˆå€¼')
    parser.add_argument('--device', type=str, default='auto', 
                       help='è¿è¡Œè®¾å¤‡')
    parser.add_argument('--save-video', type=str, 
                       help='ä¿å­˜æ£€æµ‹ç»“æœè§†é¢‘çš„è·¯å¾„')
    parser.add_argument('--no-show', action='store_true', 
                       help='ä¸æ˜¾ç¤ºæ£€æµ‹ç»“æœçª—å£')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    detector = YOLOv8FaceDetector(
        model_path=args.model,
        conf_threshold=args.conf,
        device=args.device
    )
    
    # åˆ¤æ–­è¾“å…¥ç±»å‹å¹¶å¤„ç†
    source = args.source
    
    # å¦‚æœæ˜¯æ‘„åƒå¤´
    if source.isdigit():
        detector.detect_video(
            source=int(source),
            save_path=args.save_video,
            show=not args.no_show
        )
    
    # å¦‚æœæ˜¯æ–‡ä»¶æˆ–ç›®å½•
    else:
        source_path = Path(source)
        
        if not source_path.exists():
            print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {source}")
            return
        
        # è§†é¢‘æ–‡ä»¶
        if source_path.is_file() and source_path.suffix.lower() in ['.mp4', '.avi', '.mov', '.mkv']:
            detector.detect_video(
                source=str(source_path),
                save_path=args.save_video,
                show=not args.no_show
            )
        
        # å›¾ç‰‡æ–‡ä»¶
        elif source_path.is_file() and source_path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']:
            image = cv2.imread(str(source_path))
            faces, vis_image = detector.detect_faces(image, visualize=True)
            
            # ä¿å­˜ç»“æœ
            output_dir = Path(args.output)
            output_dir.mkdir(parents=True, exist_ok=True)
            output_file = output_dir / f"detected_{source_path.name}"
            cv2.imwrite(str(output_file), vis_image)
            
            print(f"âœ… æ£€æµ‹åˆ° {len(faces)} ä¸ªäººè„¸")
            print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            
            # æ˜¾ç¤ºç»“æœ
            if not args.no_show:
                cv2.imshow('æ£€æµ‹ç»“æœ', vis_image)
                cv2.waitKey(0)
                cv2.destroyAllWindows()
        
        # å›¾ç‰‡ç›®å½•
        elif source_path.is_dir():
            detector.batch_detect(str(source_path), args.output)
        
        else:
            print(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {source}")


if __name__ == '__main__':
    main()
