#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é¢„å¤„ç†å’Œæ ¼å¼è½¬æ¢å·¥å…·
æ”¯æŒå¤šç§äººè„¸æ•°æ®é›†æ ¼å¼è½¬æ¢ä¸ºYOLOæ ¼å¼
"""

import os
import cv2
import json
import xml.etree.ElementTree as ET
import numpy as np
from pathlib import Path
import pandas as pd
from PIL import Image
import argparse
from tqdm import tqdm
import shutil
import random


class DataProcessor:
    """æ•°æ®å¤„ç†å™¨åŸºç±»"""
    
    def __init__(self, input_dir, output_dir):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºYOLOæ ¼å¼ç›®å½•ç»“æ„
        self.images_dir = self.output_dir / 'images'
        self.labels_dir = self.output_dir / 'labels'
        self.images_dir.mkdir(parents=True, exist_ok=True)
        self.labels_dir.mkdir(parents=True, exist_ok=True)
    
    def normalize_bbox(self, bbox, img_width, img_height):
        """
        å°†è¾¹ç•Œæ¡†åæ ‡æ ‡å‡†åŒ–ä¸ºYOLOæ ¼å¼ (x_center, y_center, width, height)
        
        Args:
            bbox: è¾¹ç•Œæ¡† [x1, y1, x2, y2] æˆ– [x1, y1, width, height]
            img_width: å›¾åƒå®½åº¦
            img_height: å›¾åƒé«˜åº¦
            
        Returns:
            tuple: YOLOæ ¼å¼çš„å½’ä¸€åŒ–åæ ‡ (x_center, y_center, width, height)
        """
        x1, y1, w_or_x2, h_or_y2 = bbox
        
        # åˆ¤æ–­è¾“å…¥æ ¼å¼å¹¶è½¬æ¢ä¸º [x1, y1, x2, y2]
        if w_or_x2 > img_width or h_or_y2 > img_height:
            # å¾ˆå¯èƒ½æ˜¯ [x1, y1, width, height] æ ¼å¼
            x2 = x1 + w_or_x2
            y2 = y1 + h_or_y2
        else:
            # å¯èƒ½æ˜¯ [x1, y1, x2, y2] æ ¼å¼
            x2, y2 = w_or_x2, h_or_y2
        
        # ç¡®ä¿åæ ‡åœ¨æœ‰æ•ˆèŒƒå›´å†…
        x1 = max(0, min(x1, img_width))
        y1 = max(0, min(y1, img_height))
        x2 = max(0, min(x2, img_width))
        y2 = max(0, min(y2, img_height))
        
        # è®¡ç®—ä¸­å¿ƒç‚¹å’Œå°ºå¯¸
        width = x2 - x1
        height = y2 - y1
        center_x = x1 + width / 2
        center_y = y1 + height / 2
        
        # å½’ä¸€åŒ–
        norm_x = center_x / img_width
        norm_y = center_y / img_height
        norm_width = width / img_width
        norm_height = height / img_height
        
        return norm_x, norm_y, norm_width, norm_height
    
    def save_yolo_annotation(self, image_name, bboxes, class_id=0):
        """
        ä¿å­˜YOLOæ ¼å¼çš„æ ‡æ³¨æ–‡ä»¶
        
        Args:
            image_name (str): å›¾åƒæ–‡ä»¶å
            bboxes (list): è¾¹ç•Œæ¡†åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (x_center, y_center, width, height)
            class_id (int): ç±»åˆ«ID
        """
        annotation_file = self.labels_dir / f"{Path(image_name).stem}.txt"
        
        with open(annotation_file, 'w') as f:
            for bbox in bboxes:
                x_center, y_center, width, height = bbox
                f.write(f"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\n")


class WIDERFaceProcessor(DataProcessor):
    """WIDER FACEæ•°æ®é›†å¤„ç†å™¨"""
    
    def process(self, annotation_file, split_name='train'):
        """
        å¤„ç†WIDER FACEæ•°æ®é›†
        
        Args:
            annotation_file (str): æ ‡æ³¨æ–‡ä»¶è·¯å¾„
            split_name (str): æ•°æ®é›†åˆ†å‰²åç§°
        """
        print(f"ğŸ”„ å¤„ç†WIDER FACEæ•°æ®é›†: {split_name}")
        
        with open(annotation_file, 'r') as f:
            lines = f.readlines()
        
        i = 0
        processed_count = 0
        
        with tqdm(total=len(lines), desc=f"å¤„ç†{split_name}é›†") as pbar:
            while i < len(lines):
                # è¯»å–å›¾åƒæ–‡ä»¶å
                img_name = lines[i].strip()
                if not img_name.endswith(('.jpg', '.png', '.jpeg')):
                    i += 1
                    pbar.update(1)
                    continue
                
                i += 1
                
                # è¯»å–äººè„¸æ•°é‡
                if i >= len(lines):
                    break
                
                try:
                    face_count = int(lines[i].strip())
                except:
                    i += 1
                    pbar.update(1)
                    continue
                
                i += 1
                
                # è¯»å–å›¾åƒ
                img_path = self.input_dir / 'images' / img_name
                if not img_path.exists():
                    # å°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„
                    img_path = self.input_dir / img_name
                
                if not img_path.exists():
                    # è·³è¿‡ä¸å­˜åœ¨çš„å›¾åƒ
                    i += face_count
                    pbar.update(1)
                    continue
                
                try:
                    image = cv2.imread(str(img_path))
                    if image is None:
                        i += face_count
                        pbar.update(1)
                        continue
                    
                    img_height, img_width = image.shape[:2]
                except:
                    i += face_count
                    pbar.update(1)
                    continue
                
                # è¯»å–è¾¹ç•Œæ¡†
                bboxes = []
                for j in range(face_count):
                    if i >= len(lines):
                        break
                    
                    parts = lines[i].strip().split()
                    if len(parts) >= 4:
                        try:
                            x, y, w, h = map(float, parts[:4])
                            # è½¬æ¢ä¸ºYOLOæ ¼å¼
                            norm_bbox = self.normalize_bbox([x, y, w, h], img_width, img_height)
                            bboxes.append(norm_bbox)
                        except:
                            pass
                    
                    i += 1
                    pbar.update(1)
                
                # ä¿å­˜å¤„ç†åçš„æ•°æ®
                if bboxes:
                    # å¤åˆ¶å›¾åƒ
                    output_img_path = self.images_dir / f"{split_name}_{Path(img_name).name}"
                    shutil.copy2(img_path, output_img_path)
                    
                    # ä¿å­˜æ ‡æ³¨
                    self.save_yolo_annotation(output_img_path.name, bboxes, class_id=0)
                    processed_count += 1
        
        print(f"âœ… å¤„ç†å®Œæˆ: {processed_count} å¼ å›¾åƒ")


class COCOProcessor(DataProcessor):
    """COCOæ ¼å¼æ•°æ®é›†å¤„ç†å™¨"""
    
    def process(self, annotation_file, image_dir=None):
        """
        å¤„ç†COCOæ ¼å¼æ•°æ®é›†
        
        Args:
            annotation_file (str): COCOæ ‡æ³¨JSONæ–‡ä»¶è·¯å¾„
            image_dir (str): å›¾åƒç›®å½•è·¯å¾„
        """
        print("ğŸ”„ å¤„ç†COCOæ ¼å¼æ•°æ®é›†")
        
        if image_dir is None:
            image_dir = self.input_dir / 'images'
        else:
            image_dir = Path(image_dir)
        
        # è¯»å–COCOæ ‡æ³¨æ–‡ä»¶
        with open(annotation_file, 'r') as f:
            coco_data = json.load(f)
        
        # åˆ›å»ºå›¾åƒIDåˆ°æ–‡ä»¶åçš„æ˜ å°„
        image_info = {img['id']: img for img in coco_data['images']}
        
        # åˆ›å»ºç±»åˆ«æ˜ å°„ (å‡è®¾äººè„¸ç±»åˆ«IDä¸º1æˆ–è€…'person')
        face_categories = []
        for cat in coco_data['categories']:
            if 'face' in cat['name'].lower() or 'person' in cat['name'].lower():
                face_categories.append(cat['id'])
        
        if not face_categories:
            print("âš ï¸  æœªæ‰¾åˆ°äººè„¸ç›¸å…³ç±»åˆ«ï¼Œä½¿ç”¨æ‰€æœ‰ç±»åˆ«")
            face_categories = [cat['id'] for cat in coco_data['categories']]
        
        # å¤„ç†æ ‡æ³¨
        processed_images = set()
        
        for annotation in tqdm(coco_data['annotations'], desc="å¤„ç†æ ‡æ³¨"):
            if annotation['category_id'] not in face_categories:
                continue
            
            image_id = annotation['image_id']
            if image_id not in image_info:
                continue
            
            image_data = image_info[image_id]
            img_name = image_data['file_name']
            img_path = image_dir / img_name
            
            if not img_path.exists():
                continue
            
            # è·å–å›¾åƒå°ºå¯¸
            img_width = image_data['width']
            img_height = image_data['height']
            
            # å¤„ç†è¾¹ç•Œæ¡†
            bbox = annotation['bbox']  # [x, y, width, height]
            norm_bbox = self.normalize_bbox(bbox, img_width, img_height)
            
            # å¤åˆ¶å›¾åƒ (åªå¤åˆ¶ä¸€æ¬¡)
            if image_id not in processed_images:
                output_img_path = self.images_dir / img_name
                shutil.copy2(img_path, output_img_path)
                processed_images.add(image_id)
            
            # ä¿å­˜æ ‡æ³¨ (è¿½åŠ æ¨¡å¼)
            annotation_file = self.labels_dir / f"{Path(img_name).stem}.txt"
            with open(annotation_file, 'a') as f:
                x_center, y_center, width, height = norm_bbox
                f.write(f"0 {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\n")
        
        print(f"âœ… å¤„ç†å®Œæˆ: {len(processed_images)} å¼ å›¾åƒ")


class PascalVOCProcessor(DataProcessor):
    """Pascal VOCæ ¼å¼æ•°æ®é›†å¤„ç†å™¨"""
    
    def process(self, annotations_dir, images_dir=None):
        """
        å¤„ç†Pascal VOCæ ¼å¼æ•°æ®é›†
        
        Args:
            annotations_dir (str): XMLæ ‡æ³¨æ–‡ä»¶ç›®å½•
            images_dir (str): å›¾åƒæ–‡ä»¶ç›®å½•
        """
        print("ğŸ”„ å¤„ç†Pascal VOCæ ¼å¼æ•°æ®é›†")
        
        annotations_dir = Path(annotations_dir)
        if images_dir is None:
            images_dir = self.input_dir / 'images'
        else:
            images_dir = Path(images_dir)
        
        xml_files = list(annotations_dir.glob('*.xml'))
        processed_count = 0
        
        for xml_file in tqdm(xml_files, desc="å¤„ç†XMLæ–‡ä»¶"):
            try:
                # è§£æXML
                tree = ET.parse(xml_file)
                root = tree.getroot()
                
                # è·å–å›¾åƒä¿¡æ¯
                filename = root.find('filename').text
                img_path = images_dir / filename
                
                if not img_path.exists():
                    continue
                
                size = root.find('size')
                img_width = int(size.find('width').text)
                img_height = int(size.find('height').text)
                
                # å¤„ç†æ ‡æ³¨å¯¹è±¡
                bboxes = []
                for obj in root.findall('object'):
                    class_name = obj.find('name').text.lower()
                    
                    # åªå¤„ç†äººè„¸ç›¸å…³ç±»åˆ«
                    if 'face' not in class_name and 'person' not in class_name and 'head' not in class_name:
                        continue
                    
                    # è·å–è¾¹ç•Œæ¡†
                    bbox_elem = obj.find('bndbox')
                    xmin = float(bbox_elem.find('xmin').text)
                    ymin = float(bbox_elem.find('ymin').text)
                    xmax = float(bbox_elem.find('xmax').text)
                    ymax = float(bbox_elem.find('ymax').text)
                    
                    # è½¬æ¢ä¸ºYOLOæ ¼å¼
                    norm_bbox = self.normalize_bbox([xmin, ymin, xmax, ymax], img_width, img_height)
                    bboxes.append(norm_bbox)
                
                # ä¿å­˜å¤„ç†åçš„æ•°æ®
                if bboxes:
                    # å¤åˆ¶å›¾åƒ
                    output_img_path = self.images_dir / filename
                    shutil.copy2(img_path, output_img_path)
                    
                    # ä¿å­˜æ ‡æ³¨
                    self.save_yolo_annotation(filename, bboxes, class_id=0)
                    processed_count += 1
                    
            except Exception as e:
                print(f"âš ï¸  å¤„ç†æ–‡ä»¶ {xml_file} æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"âœ… å¤„ç†å®Œæˆ: {processed_count} å¼ å›¾åƒ")


class DataSplitter:
    """æ•°æ®é›†åˆ†å‰²å·¥å…·"""
    
    @staticmethod
    def split_dataset(data_dir, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1, seed=42):
        """
        å°†æ•°æ®é›†åˆ†å‰²ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†
        
        Args:
            data_dir (str): æ•°æ®ç›®å½•è·¯å¾„
            train_ratio (float): è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio (float): éªŒè¯é›†æ¯”ä¾‹  
            test_ratio (float): æµ‹è¯•é›†æ¯”ä¾‹
            seed (int): éšæœºç§å­
        """
        data_dir = Path(data_dir)
        images_dir = data_dir / 'images'
        labels_dir = data_dir / 'labels'
        
        if not images_dir.exists() or not labels_dir.exists():
            print("âŒ æ•°æ®ç›®å½•ç»“æ„ä¸æ­£ç¡®")
            return
        
        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
        image_files = []
        for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:
            image_files.extend(images_dir.glob(ext))
        
        if not image_files:
            print("âŒ æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶")
            return
        
        # è¿‡æ»¤æœ‰å¯¹åº”æ ‡æ³¨æ–‡ä»¶çš„å›¾åƒ
        valid_images = []
        for img_file in image_files:
            label_file = labels_dir / f"{img_file.stem}.txt"
            if label_file.exists():
                valid_images.append(img_file)
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(valid_images)} ä¸ªæœ‰æ•ˆçš„å›¾åƒ-æ ‡æ³¨å¯¹")
        
        # éšæœºæ‰“ä¹±
        random.seed(seed)
        random.shuffle(valid_images)
        
        # è®¡ç®—åˆ†å‰²ç‚¹
        total_count = len(valid_images)
        train_count = int(total_count * train_ratio)
        val_count = int(total_count * val_ratio)
        
        # åˆ†å‰²æ•°æ®
        train_files = valid_images[:train_count]
        val_files = valid_images[train_count:train_count + val_count]
        test_files = valid_images[train_count + val_count:]
        
        print(f"ğŸ“ˆ æ•°æ®åˆ†å‰²: è®­ç»ƒé›†={len(train_files)}, éªŒè¯é›†={len(val_files)}, æµ‹è¯•é›†={len(test_files)}")
        
        # åˆ›å»ºåˆ†å‰²åçš„ç›®å½•ç»“æ„
        for split_name, file_list in [('train', train_files), ('val', val_files), ('test', test_files)]:
            if not file_list:
                continue
                
            split_images_dir = data_dir / split_name / 'images'
            split_labels_dir = data_dir / split_name / 'labels'
            split_images_dir.mkdir(parents=True, exist_ok=True)
            split_labels_dir.mkdir(parents=True, exist_ok=True)
            
            # å¤åˆ¶æ–‡ä»¶
            for img_file in tqdm(file_list, desc=f"å¤åˆ¶{split_name}é›†"):
                # å¤åˆ¶å›¾åƒ
                shutil.copy2(img_file, split_images_dir / img_file.name)
                
                # å¤åˆ¶æ ‡æ³¨
                label_file = labels_dir / f"{img_file.stem}.txt"
                shutil.copy2(label_file, split_labels_dir / f"{img_file.stem}.txt")
        
        print("âœ… æ•°æ®é›†åˆ†å‰²å®Œæˆ!")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='äººè„¸æ•°æ®é›†é¢„å¤„ç†å·¥å…·')
    parser.add_argument('--format', type=str, required=True,
                       choices=['wider', 'coco', 'voc'],
                       help='è¾“å…¥æ•°æ®é›†æ ¼å¼')
    parser.add_argument('--input-dir', type=str, required=True,
                       help='è¾“å…¥æ•°æ®ç›®å½•')
    parser.add_argument('--output-dir', type=str, required=True,
                       help='è¾“å‡ºæ•°æ®ç›®å½•')
    parser.add_argument('--annotation-file', type=str,
                       help='æ ‡æ³¨æ–‡ä»¶è·¯å¾„ (WIDER/COCOæ ¼å¼éœ€è¦)')
    parser.add_argument('--image-dir', type=str,
                       help='å›¾åƒç›®å½•è·¯å¾„ (å¯é€‰)')
    parser.add_argument('--split-data', action='store_true',
                       help='æ˜¯å¦åˆ†å‰²æ•°æ®é›†')
    parser.add_argument('--train-ratio', type=float, default=0.7,
                       help='è®­ç»ƒé›†æ¯”ä¾‹')
    parser.add_argument('--val-ratio', type=float, default=0.2,
                       help='éªŒè¯é›†æ¯”ä¾‹')
    parser.add_argument('--test-ratio', type=float, default=0.1,
                       help='æµ‹è¯•é›†æ¯”ä¾‹')
    
    args = parser.parse_args()
    
    # éªŒè¯å‚æ•°
    if args.format in ['wider', 'coco'] and not args.annotation_file:
        print("âŒ WIDER FACEå’ŒCOCOæ ¼å¼éœ€è¦æä¾›æ ‡æ³¨æ–‡ä»¶è·¯å¾„")
        return
    
    # åˆ›å»ºå¤„ç†å™¨
    if args.format == 'wider':
        processor = WIDERFaceProcessor(args.input_dir, args.output_dir)
        processor.process(args.annotation_file)
    elif args.format == 'coco':
        processor = COCOProcessor(args.input_dir, args.output_dir)
        processor.process(args.annotation_file, args.image_dir)
    elif args.format == 'voc':
        processor = PascalVOCProcessor(args.input_dir, args.output_dir)
        annotations_dir = args.annotation_file or (Path(args.input_dir) / 'annotations')
        processor.process(annotations_dir, args.image_dir)
    
    # æ•°æ®é›†åˆ†å‰²
    if args.split_data:
        DataSplitter.split_dataset(
            data_dir=args.output_dir,
            train_ratio=args.train_ratio,
            val_ratio=args.val_ratio,
            test_ratio=args.test_ratio
        )
    
    print("ğŸ‰ æ•°æ®é¢„å¤„ç†å®Œæˆ!")


if __name__ == '__main__':
    main()
